[ { "title": "Hadoop : Map/Reduce Using Python", "url": "/posts/hadoop-map-reduce-using-python/", "categories": "microservcies", "tags": "apache, docker, solution architect, hive", "date": "2025-09-22 00:00:00 +0800", "snippet": "Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the answer to the unevitable question we face one day or another as we live in a data age – which is : how do we process tons of data efficiently ? It is not just about storage, but also, and even more, about implementing data processing models that can provide insights to decision makers in a competitive world – where everything has to be fast and resilient.There are many important concepts to know in order to understand the hadoop framework – in this tutorial we will focus on 2 of them (the interested reader can check this article for an extensive review of hadoop concepts) : Map/Reduce model : a programming model for data processing, inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal. Map/Reduce program can be written in several popular languages – Java, Python, Ruby etc. – or wrapped using distributed tools, like Apache Hive, built on top of the hadoop platform. HDFS : Hadoop comes with a distributed filesystem called HDFS, which stands for Hadoop Distributed Filesystem. HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardwareKanmeugne’s Blog – Hadoop : Map/Reduce Using PythonIn this tutorial, you’ll learn how to process data in HDFS using Python, and then use Hive to query your results directly from the Hadoop cluster. Whether you’re new to Hadoop or looking to experiment with distributed data processing and analytics, this tutorial offers a practical, reproducible starting point right from your local machine.Ready to dive in? Just clone the repo, follow the step-by-step instructions, and start exploring big data with Hadoop, MapReduce, and Hive — all powered by Docker.How toBuild the applicationThe full code for this tutorial is available from github, you just have to pull and run : Clone and deploy the Hadoop Docker setup: $ git clone https://github.com/kanmeugne/modern-data-architectures.git $ cd modern-data-architectures/handzon-hadoop-python-map-reduce $ docker-compose up -d # [+] Building 0.0s ... # ...This launches namenodes, datanodes, and supporting services in containers. It also creates a hive server, to create and explore an hdfs-compatible database.Add some data in the hdfs serverLet’s add some data in the distributed filesystem: Copy your CSV file into the namenode container:$ curl -L -o movieratings.csv https://files.grouplens.org/datasets/movielens/ml-100k/u.data$ docker cp movieratings.csv &lt;namenode-container&gt;:/tmp/ # on the docker host The dataset comes from GroupLens, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities specializing in recommender systems, online communities, mobile and ubiquitous technologies, digital libraries, and local geographic information systems. Load the CSV into an HDFS folder within the container:$ docker exec &lt;namenode-container&gt; hdfs dfs -mkdir -p /input$ docker exec &lt;namenode-container&gt; hdfs dfs -put /tmp/movieratings.csv /input/ # in the dockerTest the mapper and reducer functions on the hostIt is possible to test the python scripts using the console pipe.$ cat movieratings.csv | python mapper.py | python reducer.py...708 4.0566 4.01010 4.050 5.0134 5.0... Copy the mapper.py and reducer.py files in the namenode container $ docker cp mapper.py &lt;namenode-container&gt;:/tmp/$ docker cp reducer.py &lt;namenode-container&gt;:/tmp/ Mapper (mapper.py): #!/usr/bin/env python3import sysfor line in sys.stdin:_, movie_id, rating, _ = line.strip().split('\\t')print(movie_id+'\\t'+rating) Explanation: For each line, output the movie ID as key and the rating as value. Reducer (reducer.py): #!/usr/bin/env python3import syscurrent_movie = Noneratings = []for line in sys.stdin: movie_id, rating = line.strip().split('\\t') if current_movie and movie_id != current_movie: print(current_movie+'\\t'+str(round(sum(ratings)/len(ratings), 2))) ratings = [] current_movie = movie_id ratings.append(float(rating))if current_movie: print(current_movie+'\\t'+str(round(sum(ratings)/len(ratings), 2))) Explanation: For each movie, compute and output the average of its ratings. Run the MapReduce JobNow that we have tested the map/reduce python script on the terminal, we can now run the scripts on the hadoop nodes. docker exec &lt;namenode-container&gt; \\ hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\ -file /tmp/mapper.py -mapper /tmp/mapper.py \\ -file /tmp/reducer.py -reducer /tmp/reducer.py \\ -input /input/movieratings.csv \\ -output /output # 2025-05-05 22:48:57,076 WARN streaming.StreamJob... # ... # 2025-05-05 22:49:05,815 INFO mapreduce.Job: Job job... # 2025-05-05 22:49:05,816 INFO mapreduce.Job: map 0% reduce 0% # 2025-05-05 22:49:11,892 INFO mapreduce.Job: map 50% reduce 0% # 2025-05-05 22:49:12,901 INFO mapreduce.Job: map 100% reduce 0% # 2025-05-05 22:49:16,927 INFO mapreduce.Job: map 100% reduce 100% # 2025-05-05 22:49:16,936 INFO mapreduce.Job: Job .... completed successfully # 2025-05-05 22:49:17,032 INFO mapreduce.Job: Counters: 54 # ... # 2025-05-05 22:49:17,032 INFO streaming.StreamJob: Output directory: /outputUse a hive serverYou can query the data from a hive server that runs on the hadoop cluster nodes : Start the Hive server (from any cluster node): $ docker exec -it &lt;hive-server-container&gt; bash root@xxx:/opt/ beeline -u jdbc:hive2://localhost:10000 # SLF4J: Class path contains multiple SLF4J bindings. # SLF4J: Found binding in ... # SLF4J: Found binding in ... # SLF4J: See ... # SLF4J: Actual binding is of type ... # Connecting to jdbc:hive2://localhost:10000 # Connected to: Apache Hive (version 2.3.2) # ... # Beeline version 2.3.2 by Apache Hive 0: jdbc:hive2://localhost:10000&gt; Create an external table for results: beeline&gt; CREATE EXTERNAL TABLE movie_avg_rating ( movie_id STRING, avg_rating FLOAT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LOCATION '/output'; Query results: SELECT * FROM movie_avg_rating ORDER BY avg_rating DESC LIMIT 4;+----------+-----------+---------+-----------+| user_id | movie_id | rating | datation |+----------+-----------+---------+-----------+| 196 | 242 | 3.0 | 881250949 || 186 | 302 | 3.0 | 891717742 || 305 | 451 | 3.0 | 886324817 || 6 | 86 | 3.0 | 883603013 |+----------+-----------+---------+-----------+Compare job durations as you increase the number of DataNodes.You can increase the number of nodes and confirm the following speed-ups. Nodes Example Time (s) Notes 1 120 Single DataNode 2 75 Parallel processing 3 55 Further speedup ConclusionThank you for your attention. Feel free to share this tutorial and to send your comments." }, { "title": "Hadoop : query data from a self-hosted hadoop cluster", "url": "/posts/hadoop-and-hive/", "categories": "microservcies", "tags": "apache, superset, postgresql, docker, solution architect, hive", "date": "2025-05-15 00:00:00 +0800", "snippet": "Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the answer to the unevitable question we face one day or another as we live in a data age – which is : how do we process tons of data efficiently ? It is not just about storage, but also, and even more, about implementing data processing models that can provide insights to decision makers in a competitive world – where everything has to be fast and resilient.Kanmeugne’s Blog – Hadoop : query data from a self-hosted hadoop clusterThere are many important concepts to know in order to understand the hadoop framework – in this tutorial we will focus on 3 of them : Map/Reduce model : a programming model for data processing, inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal. Map/Reduce program can be written in several popular languages – Java, Python, Ruby etc. – or wrapped using distributed tools, like Apache Hive, built on top of the hadoop platform. HDFS : Hadoop comes with a distributed filesystem called HDFS, which stands for Hadoop Distributed Filesystem. HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware Apache YARN (Yet Another Resource Negotiator) is Hadoop’s cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Instead, users write to higher-level APIs provided by distributed computing frameworks, which themselves are built on YARN and hide the resource management details from the userTo learn more about hadoop platform, the interested reader could have a look at this excellent book, published by Oreilly.How ToNow let’s jump to the hands on tutorial. I will mostly focus on high level operations – data i/o and analysis – for the interested users could easily find more specific tutorials on low-level operations. Here, we will rapidly set up a custom cluster using docker compose, add some data in the corresponding HDFS filesystem, and process the data using Hive.Set the self-hosted clusterThe full code for this tutorial is available from github, you just have to pull and run : Clone and deploy the Hadoop Docker setup: $ git clone https://github.com/kanmeugne/modern-data-architectures.git$ cd modern-data-architectures/handzon-hadoop-hive $ docker-compose up -d This launches namenodes, datanodes, and supporting services in containers. It also creates a hive server, to create and query data in a hdfs-compatible database. Check running containers: $ docker ps All Hadoop containers (namenode, datanode(s), etc.) should be listed. Check hdfs filesystem from inside the name node: $ docker exec &lt;namenode&gt; hdfs dfsadmin -report # this command lists all live DataNodes connected to the cluster.Configured Capacity: *** (*** GB)Present Capacity: *** (*** GB)DFS Remaining: *** (*** GB)DFS Used: *** (*** MB)DFS Used%: 0.01%Replicated Blocks: Under replicated blocks: 6 Blocks with corrupt replicas: 0 Missing blocks: 0 Missing blocks (with replication factor 1): 0 Low redundancy blocks with highest priority to recover: 6 Pending deletion blocks: 0Erasure Coded Block Groups: Low redundancy block groups: 0 Block groups with corrupt internal blocks: 0 Missing block groups: 0 Low redundancy blocks with highest priority to recover: 0 Pending deletion blocks: 0-------------------------------------------------Live datanodes (1):Name: *** (datanode.handzon-hadoop-hive_hadoop_network)Hostname: e20decb5140eDecommission Status : NormalConfigured Capacity: *** (*** GB)DFS Used: *** (*** MB)Non DFS Used: *** (*** GB)DFS Remaining: *** (*** GB)DFS Used%: 0.00%DFS Remaining%: 5.85%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Fri May 09 20:23:16 UTC 2025Last Block Report: Fri May 09 20:17:40 UTC 2025Num of Blocks: 6 ... Add data in the clusterLet’s add some data in the distributed filesystem: Copy your CSV file into the namenode container: $ curl -L -o movieratings.csv https://files.grouplens.org/datasets/movielens/ml-100k/u.data$ docker cp movieratings.csv &lt;namenode&gt;:/tmp/ # on the docker host The dataset comes from GroupLens, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities specializing in recommender systems, online communities, mobile and ubiquitous technologies, digital libraries, and local geographic information systems. Load the CSV into an HDFS folder within the container: $ docker exec &lt;namenode&gt; hdfs dfs -mkdir -p /input$ docker exec &lt;namenode&gt; hdfs dfs -put /tmp/movieratings.csv /input/ # in the docker Explore your data with HiveUsing Hive you can explore data with SQL-like queries : Access the Hive service container $ docker exec -it &lt;hive-server&gt; bash # `&lt;hive-server&gt;` is the name of your hive server Create an external table from the HDFS file: # in the docker$ beeline -u jdbc:hive2://localhost:10000...Connecting to jdbc:hive2://localhost:10000Connected to: Apache Hive (version 2.3.2)Driver: Hive JDBC (version 2.3.2)Transaction isolation: TRANSACTION_REPEATABLE_READBeeline version 2.3.2 by Apache Hive... 0: jdbc:hive2://localhost:10000&gt;# This tells Hive to use the CSV at `/input` in HDFS as the data source.CREATE EXTERNAL TABLE IF NOT EXISTS movieratins ( user_id STRING, movie_id STRING, rating FLOAT, datation STRING) ROW FORMATDELIMITED FIELDS TERMINATED BY '\\t'STORED AS TEXTFILELOCATION '/input'; # hit enter # You should see this message after you hit `enter`No rows affected (1.629 seconds) Query the created table 0: hive2://localhost:10000&gt; select * from movieratings limit 4; # hit enter +----------+-----------+---------+-----------+| user_id | movie_id | rating | datation |+----------+-----------+---------+-----------+| 196 | 242 | 3.0 | 881250949 || 186 | 302 | 3.0 | 891717742 || 305 | 451 | 3.0 | 886324817 || 6 | 86 | 3.0 | 883603013 |+----------+-----------+---------+-----------+ Do some analytics using sql queries on the hive table # compute the average rating per movie0: jdbc:hive2://localhost:10000&gt; SELECT movie_id,AVG(rating) as ratingFROM movieratingGROU BY movie_idORDER BY LENGTH(movie_id), movie_idLIMIT 10; # results+-----------+---------------------+| movie_id | rating |+-----------+---------------------+| 1 | 3.8783185840707963 || 2 | 3.2061068702290076 || 3 | 3.033333333333333 || 4 | 3.550239234449761 || 5 | 3.302325581395349 || 6 | 3.576923076923077 || 7 | 3.798469387755102 || 8 | 3.9954337899543377 || 9 | 3.8963210702341136 || 10 | 3.831460674157303 |+-----------+---------------------+10 rows selected (2.909 seconds) 0: jdbc:hive2://localhost:10000&gt; !quit Voilà! You can add nodes and compare execution timesFeel free to pull this repo and to send me your comments/remarks." }, { "title": "Apache Superset and Postgresql : connecting your database to a powerful data visualisation engine", "url": "/posts/postgresql-and-apache-superset/", "categories": "microservcies", "tags": "apache, superset, postgresql, docker, solution architect", "date": "2025-05-01 00:00:00 +0800", "snippet": "Data is only as valuable as the insights you can extract from it — and in today’s world, those insights need to be fast, interactive, and visually compelling.Kanmeugne’s Blog : Apache Superset and Postgresql — connecting your database to a powerful data visualisation engineApache Superset, a powerful open-source data visualization platform, is rapidly becoming the go-to tool for analysts and developers who want to turn raw data into actionable dashboards without wrestling with complex setup processes. Thanks to Docker and Docker Compose, spinning up a full-featured Superset environment takes just a few minutes, letting you focus on what matters: connecting to your data and building beautiful, shareable dashboards.Kanmeugne’s Blog : Apache Superset and Postgresql — connecting your database to a powerful data visualisation engineIn this hands-on guide, you’ll see how convenient it is to connect Superset to a PostgreSQL database using Docker Compose. Once you’re comfortable with Docker basics, Superset’s streamlined workflow makes dashboard creation not just possible, but enjoyable-even for complex data sources.I will assume you have installed Docker and Docker-compose on your computer — follow this installation guide if you haven’t, and come back to the tutorial after.Build and run the application pull the application from the github repository$ git clone https://github.com/kanmeugne/modern-data-architectures.git$ cd modern-data-architecture/handzon-apache-superset create a .env with the required environment variables for the project :# postgres variablesPOSTGRES_USER=***POSTGRES_PASSWORD=***POSTGRES_DB=***POSTGRES_PORT=***# pgadmin variablesPGADMIN_DEFAULT_EMAIL=***PGADMIN_DEFAULT_PASSWORD=***PGADMIN_PORT=***# superset variableSUPERSET_SECRET_KEY=***SUPERSET_PORT=***SUPERSET_ADMIN_USERNAME=***SUPERSET_ADMIN_PASSWORD=***SUPERSET_ADMIN_EMAIL=***SUPERSET_ADMIN_FIRST_NAME=***SUPERSET_ADMIN_LAST_NAME=*** download the dataset and save the file into source_data (you need to do it before building the app) :handzon-apache-superset/source_data$ curl -L -o brazilian-ecommerce.zip \\ https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce handzon-apache-superset/source_data$ unzip brazilian-ecommerce.zip...handzon-apache-superset/source_data$ rm brazilian-ecommerce.zipData model of the Brazilian E-Commerce Public Dataset by Olist build the application with docker compose from the project folder :handzon-apache-superset$ docker compose up -d...The build process might be long at first run, so be patient…Database walkthroughWhen everything is up, use your favorite browser and do the following checkups : check the database content by opening the pgadmin web endpoint in your browser : http://localhost:&lt;PGADMIN_PORT&gt;. You will have to log in pgadmin with the username and the password that you have defined in the .env file.# pgadmin variablesPGADMIN_DEFAULT_EMAIL=***PGADMIN_DEFAULT_PASSWORD=***PGADMIN_PORT=***PGAdmin Login Page : use the credentials defined in .env navigate in your database by setting a connection with the proper variables (the server address is the name of corresponding service container : postgis)# postgres variablesPOSTGRES_USER=***POSTGRES_PASSWORD=***POSTGRES_DB=***POSTGRES_PORT=***PGAdmin Connexion + Navigation into your sql tablesCreate analytics within SupersetTo be able to create analytics, you should at least : set a database connexion create datasets create charts and add them to dashboardsCreate a database connexionFollow these steps in order to create a connexion to your postgresql database : open the superset endpoint in your browser : http://localhost:&lt;SUPERSET_PORT&gt; and use the following credentials to sign in : SUPERSET_PORT=***SUPERSET_ADMIN_USERNAME=***SUPERSET_ADMIN_PASSWORD=*** login into Superset when signed in, go to (1) + &gt; (2) Data &gt; (3) Connect database to configure a new database connexionadd connexion to the database - select data source click on the Postgresql button since we are using a Postgresql Databaseadd connexion to the database - select database type use the postgresql credentials from the .env and click Connexion, then Finish buttons.add connexion to the database - set credential and save the connexionWithin the tool, you can now query data from the database and build the analytics upon it.Create a datasetDatasets can be created either directly from sql table or from sql queries. In this tutorial, we are going to create one from a sql query : Go to the Menu (1) SQL&gt;SQL Lab, to open the sql dataset creation wizard. Use the (2) DATABASE field to select the database connexion you just created Use (3) SCHEMA field to pick the right schema.set credentials for the dataset In the right panel, copy the following sql code to collect relevant data about orders, products and sellers SELECT oi.order_id, oi.product_id, tr.product_category_name_english as product_name, oi.price, oc.customer_cityfrom olist_order_items_dataset oi inner join olist_orders_dataset oo on oo.order_id = oi.order_id inner join olist_customers_dataset oc on oo.customer_id = oc.customer_id inner join olist_products_dataset op on oi.product_id = op.product_id left join product_category_name_translation tr on op.product_category_name = tr.product_category_name save the dataset as orders, products, sellersNow that we have created an extended dataset on orders and products, we can edit charts and dashboards to explore it.Create chartsLet’s create 3 charts that we are going to add to a dashboard later.$1^{st}$ chart : total sales per city on the dataset tab, click on orders, products, sellers drag price attribute from the left panel and drop it in the metrics cell - confirm SUM(price) as the agregation operation. drag customer_city from the left panel to the Dimensions cell. You should see the following chart (see screenshot). Save it as total sales per cityadd connexion to the database - 3$2^{nd}$ chart : number of different products per city on the dataset tab, click on orders, products, sellers use product_id as the metric (confirm COUNT_DISTINCT(product_id) as the agregation operation), and product_name as the dimension. You should see this chart (save it as number of different products per city)add connexion to the database - 3$3^{rd}$ chart : top ten sales on the dataset tab, click on orders, products, sellers use product_id as the metric (confirm COUNT_DISTINCT(product_id) as the agregation operation), and product_name as the dimension. use the ROW LIMIT option to limit the number of items to 10. You should see this chart (save it as top ten sales)add connexion to the database - 3It is now possible to agregate charts into a dashboard to have an overview of your dataCreate a dashboardDashboard creation is quite simple : on the dashboard tab, add a new dashboardcreate a new dashboard check on the right pane to see the charts you can use to build your dashboard (you should see the charts you have created above)explore charts on the right panel drag all your charts from the right pane and drop them anywhere on the dashboard canvadrag and drop charts into the dashboard save the dashboard as Product Sales View.set the dashboard title and saveYou should now see Product Sales View under the dashboard tab. Interested readers could check the Apache Superset website to see how to attach CSS template to get more compeling visuals.ConclusionI hope this tutorial will be helpful for those who want to play with Apache SuperSet and PostgreSQL. Feel free to send me your comments and remarks." }, { "title": "Setting Up Your python Environment (II)", "url": "/posts/setting-up-virtual-environments-in-python/", "categories": "software development", "tags": "python, jupyter, ipython, programming", "date": "2022-05-04 00:00:00 +0800", "snippet": "Dans un article précédent, j’expliquais en 3 étapes comment mettre en place un environnement de programmation en Python. Il existe plusieurs options d’installation en réalité (anaconda, winpython, etc.) mais j’ai privilégié la plus bas niveau ⏬.La procédure que j’ai présentée suppose l’utilisation d’un système Linux. J’ai insisté sur le fait qu’il ne s’agissait pas d’une contrainte – bien au contraire – et ce pour au moins 2 raisons : Premièrement, Linux est très populaire auprès des développeurs - le débutant pourra donc automatiquement profiter d’une importante communauté d’entraide 💃. Deuxièmement, Windows – qui est de loin l’OS le plus populaire tout court – propose des sous-systèmes Linux natifs dans ses dernières versions. Il est donc très facile de travailler sous linux aujourd’hui (encore plus que par le passé) quelque soit la version récente de Windows installée sur sa machine (pour les utilisateurs de MacOs, l’expérience montre que les procédures d’installation – au moins à partir d’un terminal – sont quasi similaires).Photo by Ralston Smith on Unsplash.Dans cet article, nous allons un peu plus loin dans l’organisation de l’espace de travail du developpeur python avec la mise en place d’environnments virtuels.Pourquoi utiliser un environnement virtuel ?Le monde des développeurs est un monde complexe, plein de contrariétés et d’épreuves.Il peut arriver par exemple : que vous ayez besoin d’une version bien précise de l’interpréteur python – différente de la version installée sur votre système – parceque la lib X que vous convoitez ne marche qu’avec cette version-là ! que, bien que vous utilisiez la version la plus récente de la lib X dans votre code Y, vous ayez besoin d’une version plus ancienne de la lib X pour qu’un autre bout de code Z – que vous avez eu tant de mal à développer – continue de fonctionner sur votre machine ! que vous ayez envie – et c’est votre droit le plus absolu – d’isoler vos projets python pour avoir une bonne vision des dépendances et des lib utilisées.Pour faire simple, vous pouvez être confronté à deux cas de figure : cohabitation : vous avez besoin de faire cohabiter plusieurs versions d’interpreteurs python ou de lib python isolement : vous voulez isoler vos projets pour avoir une bonne visibilité sur les lib et les dépendances nécessaires pour votre projet.Si vous vous retrouvez dans l’un ces deux cas de figure, vous avez certainement besoin d’utiliser un environnement virtuel.Que faut-il installer pour utiliser un environnement virtuel ?Nous allons considérer que nous sommes dans les configurations de l’article cité à l’introduction – si vous ne l’avez pas lu, faites-le et revenez vite 😼.Commencez par installer virtualenv et virtualenvwrapper qui sont des lib python qui permettent : de créer un environnement de dev isolé du reste du système (virtualenv) de gérer les environnements virtuel depuis le terminal (virtualenvwrapper)pip install virtualenv virtualenvwrappervirtualenvwrapper crée un programme – virtualenvwrapper.sh – qui doit s’exécuter à chaque début session du terminal pour définir les commandes permettant de gérer les environnements virtuels. Assurez-vous de bien le localiser et de l’exécuter dans votre ~/.bashrc pour que les commandes soient créées automatiquement à chaque session.Pensez aussi à déclarer le dossier dans lequel les environnements virtuels seront créés WORKON_DIR.Ci-dessous, un exemple de configuration.# ~/.bashrc (ou ~/.zshrc)# interpréteur par defautexport VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 # les environnements virtuels sont crées iciexport WORKON_HOME=~/.virtualenvs ## generer les commandes pour gérer les environnements virtuelssource ~/.local/bin/virtualenvwrapper.sh# pour utiliser l'environnement virtuel dès sa créationexport PIP_RESPECT_VIRTUALENV=true 💁 Pour plus d’options de configuration, bien vouloir consulter la documentation officielle.Voilà, vous êtes prêts à tester les environnements virtuels.Comment utiliser un environnement virtuel ? ⚠ Le tutoriel est basé sur virtualenv 20.2.2!$ pip show virtualenv Name: virtualenvVersion: 20.2.2Summary: Virtual python Environment builder... Premièrement, il faut en créer un… Et pour cela, vous devez utiliser la commande mkvirtualenv. Nous allons utiliser mkvirtualenv pour créer un environnement virtuel que nous allons appeler myenv.$ mkvirtualenv --python 3 myenvcreated virtual environment CPython3.8.10.final.0-64 in 200ms ...virtualenvwrapper.user_scripts creating ~/.virtualenvs/myenv/bin/predeactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myenv/bin/postdeactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myenv/bin/preactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myenv/bin/postactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myenv/bin/get_env_details(myenv) $ Vous devriez obtenir l’équivalent des logs ci-dessus. Remarquez que le prompt a légèrement changé (si tout s’est bien passé). Vous avez maintenant (myenv) avant l’invite (cf. ligne 10).À partir de maintenant, on travaille dans un espace virtuel – toutes les installations de lib se feront dans cet espace uniquement, et non sur l’ensemble du système.On peut vérifier que l’espace nouvellement créé est quasi vierge et que très peu de lib sont pré-installées (juste de quoi installer d’autres lib 😉)(myenv) $ pip listPackage Version---------- -------pip 22.1setuptools 46.1.3wheel 0.34.2Installons le prompt ipython dans notre environnement virtuel, histoire de le remplir un peu… Rien de très compliqué, il s’agit exactement des mêmes commandes que d’habitude – pip install ipython – la seule différence étant que l’installation se fait uniquement dans l’environnement virtuel.(myenv) $ pip install ipythonCollecting ipython Using cached ipython-8.3.0-py3-none-any.whl (750 kB)Collecting jedi&gt;=0.16 Using cached jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)Collecting traitlets&gt;=5 Using cached traitlets-5.2.1.post0-py3-none-any.whl (106 kB) ...Successfully installed asttokens-2.0.5 ...On peut constater que l’environnement est un peu plus chargé – ce qui est tout a fait normal car ipython a été installé, avec plusieurs autres dépendances (nécessaires au fonctionnement de ipython).(myenv) $ pip listPackage Version----------------- -----------asttokens 2.0.5backcall 0.2.0decorator 5.1.1executing 0.8.3ipython 8.3.0jedi 0.18.1matplotlib-inline 0.1.3parso 0.8.3pexpect 4.8.0pickleshare 0.7.5pip 22.1prompt-toolkit 3.0.29ptyprocess 0.7.0pure-eval 0.2.2Pygments 2.12.0setuptools 46.1.3six 1.16.0stack-data 0.2.0traitlets 5.2.1.post0wcwidth 0.2.5wheel 0.34.2Voilà! On peut maintenant utiliser ipython depuis notre environnement virtuel myenv.(myenv) ipythonPython 3.8.10 (default, Mar 15 2022, 12:22:08) Type 'copyright', 'credits' or 'license' for more informationIPython 8.3.0 -- An enhanced Interactive Python. Type '?' for help.In [1]: print(\"coucou\")coucouIn [2]: Vous pouvez sortir de l’environnement virtuel – commande deactivate – et constater que vous n’avez plus accès à ipython (sauf si vous l’aviez sur tout le système avant bien sûr. Si c’est le cas, désinstallez-le avant de suivre ce tutoriel).(myenv) $ deactivate$ ipythonbash: command not found: ipythonComment sauvegarder mon environnement virtuel ?Un des interêts des environnements virtuels c’est d’avoir une vision claire des dépendances nécessaires pour son code python. On peut ainsi reproduire son environnement de travail en toute sérénité et n’importe quand.Pour sauvegarder et cloner son environnement virtuel, on peut procéder de la manière suivante : je me connecte à l’environnement que je souhaite sauvegarder :$ workon myenv(myenv) $ je sauvegarde l’état de l’environnement dans un fichier texte, grâce notemment à la commande pip freeze. A la différence de pip list, la commande pip freeze affiche les dépendances dans un format directement exploitable pour l’installation.(myenv) $ pip freezeasttokens==2.0.5backcall==0.2.0decorator==5.1.1executing==0.8.3ipython==8.3.0jedi==0.18.1matplotlib-inline==0.1.3parso==0.8.3pexpect==4.8.0pickleshare==0.7.5prompt-toolkit==3.0.29ptyprocess==0.7.0pure-eval==0.2.2Pygments==2.12.0six==1.16.0stack-data==0.2.0traitlets==5.2.1.post0wcwidth==0.2.5(myenv) $ pip freeze &gt; requirements.txt(myenv) $ ls *.txtrequirements.txt enfin, je clone mon environnement grâce au fichier de sauvegarde requirements.txt (pour l’exemple, on supprime myenv au préalable, avec la commande rmvirtualenv)(myenv) $ deactivate$ rmvirtualenv myenvRemoving myenv...$ mkvirtualenv --python 3 -r requirements.txt myclonenvcreated virtual environment CPython3.8.10.final.0-64 in 112ms...virtualenvwrapper.user_scripts creating ~/.virtualenvs/myclonenv/bin/predeactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myclonenv/bin/postdeactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myclonenv/bin/preactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myclonenv/bin/postactivatevirtualenvwrapper.user_scripts creating ~/.virtualenvs/myclonenv/bin/get_env_detailsCollecting asttokens==2.0.5 Using cached asttokens-2.0.5-py2.py3-none-any.whl (20 kB)Collecting backcall==0.2.0 Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)...Installing collected packages: six, asttokens, ...$ (myclonenv) pip listPackage Version----------------- -----------asttokens 2.0.5backcall 0.2.0decorator 5.1.1executing 0.8.3ipython 8.3.0jedi 0.18.1matplotlib-inline 0.1.3parso 0.8.3pexpect 4.8.0pickleshare 0.7.5pip 22.1.1prompt-toolkit 3.0.29ptyprocess 0.7.0pure-eval 0.2.2Pygments 2.12.0setuptools 46.1.3six 1.16.0stack-data 0.2.0traitlets 5.2.1.post0wcwidth 0.2.5wheel 0.34.2Votre environnement est parfaitement cloné ! Et vous voilà initié à l’utilisation des environnements virtuels en python. Très utile : l’aide sur la commande mkvirtualenv – mkvirtualenv --help les documentations officielles de virtualenv et virtualenvwrapper pour plus d’options. Références Anaconda.com Winpython.github.io Virtualenv.pypa.io Virtualenvwrapper.readthedocs.io Ipython.org Kanmeugne’s Blog : Setting Up Your Python Environment" }, { "title": "Setting Up Your Python Environment (I)", "url": "/posts/setting-up-your-python-environment/", "categories": "software development", "tags": "python, jupyter, ipython, programming", "date": "2022-05-03 00:00:00 +0800", "snippet": "I have been talking python with different kind of programmers – from beginners to experienced programmers – and there is this little issue that, surprisingly, does not have a straightforward answer : what do you need to install to start programming in python ?This post is dedicated to this little issue and I will try to explain what – in my opinion – is the easiest path from nothing… to… a decent python programming environment.Photo by Amr Taha™ on Unsplash.Our journey will have several stages – and you will have to do some installation homework in order to travel from one stage to another.At the first stage, you will have to make sure that you are working on the right Operating System and with the right terminal – meaning, compatible with the instructions of this post. !!! Spoiler Alert !!! We will go for a linux OS since it is very popular among programmers and easy to set up – don’t shoot.At the second stage, you should have installed python 3 on your computer – we will see that, once installed, you technically have a working python programming environment. We will show off two programming modes at this stage : an interactive mode – where you can type commands and see the result in an interactive prompt – a script mode – where you can run full python scripts using the python interpreter from the terminal.At the third stage, we will enrich our programming environment with high-level python prompts – like ipython and jupyter – and advanced Integrated Development Environments (IDEs) – like vscode. High-level prompts give more context to programmers and are clearly a most-have if you want to improve your interactive programming experience. IDEs, as you might expect, improve the script programming experience with embedded tools for : script execution, coding style, code highlighting, testing, etc.Now, let’s begin…1. The OS : where everything startsPhoto by Gabriel Heinzer on UnsplashI will assume that you are working on a Linux machine. The first reason I am confident with this assumption is that Linux is a very popular OS for developers (beginners and experienced ones) and Microsoft (which is the most popular OS among normal human being) provides a descent linux subsystem which is an Ubuntu distribution by default. I am thus pretty sure that if the reader (this is you) is a windows user (which is highly possible), he/she will be able to set up a Linux machine, even on his/her windows computer, with little effort – the interested reader could check this article to understand how to set it up.The second reason I am confident is that, if you are a Mac user (which is highly possible), most of the setup and commands I will show in this post will work on your OS with very little or no change at all. Anyway, for the sake of straightforwardness, I will assume a linux OS – I will focus on MacOs in another post.2. The Python interpreter : the mandatory installPhoto by Alex Chumak on Unsplash Python is actually an interpreter, which means that, unlike a compiler, a python code is parsed and executed dynamically by a special program which is : the Python Interpreter.If you have a linux OS setup, it is highly possible that you already have a python shipped with your system. But you need to install the right version and make sure to be able to track it down – as you might expect, the interpreter parsing and executing behaviour depends on its version. So how do you install the right python interpreter ? Since you are working on a full linux system (as we have assumed above), you can do the following steps : Check if a python3.X.X is already installed. You can type the following command on your terminal : $ python --version If the result of this command looks like python X.Y.Z where X&lt;3 or if the command fails, you should probably install python 3.X.X. It is highly recommended since most of the python modules and tools provided by the community will not be maintained for older versions. So how do you install python 3 ? You need to install python 3, and the exact how depends on your linux distribution. For Ubuntu distributions (WSL runs Ubuntu by default), the following instructions should be sufficient (for python 3.8, but you can pick any 3.X version you want). $ sudo apt-get update $ sudo apt-get install python3.8 Anyway, I encourage 💪 you to read this post and get the right instructions for your linux distribution. Come back here when you are done with the installation. Now, you should have the right version installed and technically, you are ready to start programming in Python – you are ready to experience two different programming modes : the interactive mode and the script mode.Interactive ModeIn the interactive mode, you will be able to launch the default prompt, start typing python commands and see the results interactively by hiting the ⏎ key.To experience it now, open your linux terminal and do the following steps : Launch the python prompt $ python The above command opens the default prompt which allows you type any valid python command. For example, type the following : &gt;&gt;&gt; func = lambda x : x+1 # hit the enter key to create this lambda function &gt;&gt;&gt; print(func(6)) # hit the enter key and you should see the result below 6 Add verbosity with the following : &gt;&gt;&gt; print(\"the result of func(5) is: \", func(5)) the result of func(5) is: 6 The animation below looks almost like what you should get if you don’t mistype.Fig. Running python commands interactively using the default python promptScript ModeAs you might have guessed, in the script mode, your commands should be saved in a text file – that will be your python script. To execute it, just open your terminal and hit : python &lt;yourpythonscriptpath&gt;.For example, let’s copy the previous section’s commands in a file and try them in a script mode : Open an empty text file called test.py with your favorite editor and type in the following (make sure to save the file in your current workspace 😋) : #!/usr/bin/python3 func = lambda x: x+1 print(\"the result of func(5) is: \", func(5)) Save your file, and type the command below in a linux terminal $ python test.py the result of func(5) is: 6 or $ chmod +x test.py $ ./test.py the result of func(5) is: 6 This is it for the script mode!! All you need to do, basically, is to save your commands in a text file, and then, run it in a terminal with the appropriate python interpreter. In the next section will see how you can enrich your programming experience with advanced prompts and code editors 👌.3. Enriching your programming environmentAdvanced promptsYou can enrich your environment with advanced prompts. I am not going to review all the available prompts in this post, but there are two of them that I would like to show off.IPythonipython is an advanced prompt that you can install on your computer as a python package. All you have to do is the following in a linux terminal :$ pip install ipython --userCompared to the default python prompt, ipython is a more colorful prompt and full of helpers that allow for example to : navigate in the filesystem without exiting the prompt highlight your code (cf. animation) auto-complete commands see the available attributes and methods of an object display the commands history (cf. animation below) display the execution time of a command etc.Fig. IPython in actionJupyteripython is used as the core for even more advanced prompts like the famous web-based prompt : Jupyter Notebook.The Jupyter Notebook is the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience.As for ipython, jupyter notebook is installable as a python package :$ pip install jupyterIf the above 👆 command does not work, please go the official installation website to get the most updated procedure. Once installed, run a notebook just by typing the following command on your terminal :jupyter notebookCheck the animation below 👇 to see jupyter in actionFig. Jupyter notebook in actionIntegrated Development Environments (IDEs)Python code editors are designed for developers who want their best development tools perfectly integrated. There are editors for every type of programmers – the best ones being those you better flow with. Here is a non-exhaustive list of features one could be expecting for, while looking for the perfect IDE : a step-by-step debugger code navigation tool syntax highlighting auto completion/importation diff tools to see how the code changes between files unit/integration test tools integration etc.Below, a list of IDEs I have already seen in action (⚠ my pick is completely innocent 😁) : Visual Studio Code (or vscode) is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity) Sublime Text is a shareware cross-platform source code editor. It natively supports many programming languages and markup languages. Users can expand its functionality with plugins, typically community-built and maintained under free-software licenses. To facilitate plugins, Sublime Text features a Python API. PyCharm is an integrated development environment used in computer programming, specifically for the Python programming language. It is developed by the Czech company JetBrains (formerly known as IntelliJ). It provides code analysis, a graphical debugger, an integrated unit tester, integration with version control systems, and supports web development with Django as well as data science with Anaconda. If you want an extensive review, please check this well-documented article on python IDEs.4. References Geek4Geeks.com Python.org docs.python-guide.org/starting/install3/linux/ Guru99.com/python-ide-code-editor.html Jupyter.org Ipython.org Code.visualstudio.com Sublimetext.com JetBrains.com/PyCharm" }, { "title": "Introduction au TDD (II) : le cycle vertueux", "url": "/posts/tdd-cycle-vertueux/", "categories": "software development", "tags": "cmake, c++, tdd", "date": "2021-10-29 00:00:00 +0800", "snippet": "Dans la première partie de cet exercice, je me suis concentré sur la mise en place de l’espace de travail et la validation du test zero (qui est tout simplement la compilation). On va se concentrer maintenant sur l’implémentation des fonctionnalités de la lib témoin. On verra en quoi l’approche TDD permet d’envisager sereinement l’évolution du code et le refactoring.Photo by Liam Tucker on UnsplashLe mindsetPour rappel, le developpeur TDD respecte le cycle suivant : Écrire des petits tests S’assurer que les tests échouent dans un premier temps Écrire le code qui permet passer le test S’assurer que le test passe Nettoyer (ou réfactorer) le code S’assurer que le test passe toujours Retourner à l’étape 1.La particularité de cette approche est qu’on définit les tests en premier lieu — la production de code a pour objectif de les valider.Le premier testPour commencer la production de notre lib toolset, il faut donc prélablement définir un test qui est sensé échouer dans l’état actuel du code. Commençons par le test ci-dessous :class ParserTest : public Test{};TEST(ParserTest, Parser_LowerSingleLetter){ std::string output = \"\"; std::string input = \"L\"; std::string expected = \"l\"; MyParser parser; parser.convertToLowerCase(input, output);\tASSERT_EQ(output, \"l\");}// ... Traduction en langage naturel : La fonction convertToLowerCase du parser convertit la lettre “L” majuscule en “l” minusculeQuelques mots clefs dans cette définition nécessitent une petite explication : TEST est la macro googletest qui permet de définir un test unitaire. ASSERT_EQ est une autre MACRO qui permet de tester si 2 variables ont la même valeur (voir la documentation de googletest pour plus d’infos sur les macros disponibles). La déclaration de la classe ParserTest (dérivée de testing::Test) permet de regrouper les tests par thématique — comme on le verra plus bas, ce mécanisme permet aussi de définir des fixtures. Parser_LowerSingleLetter : est le nom du test. Très utile quand il faudra lire les résultats des tests sur la console.Le test défini ci-dessus est simple (une assertion) avec un objectif exprimable en langage naturel : le parseur doit transformer la lettre L (majuscule) en la lettre l (minuscule). Produisons maintenant le code qui permet de le valider.La première validationPour l’instant le test échoue à cause d’un problème de compilation puisque la classe MyParser n’est pas vraiment définie. En écrivant le strict minimum dans MyParser.h pour que la compilation fonctionne,#include \"MyParser.h\"//MyParser::convertToLowerCasevoid MyParser::convertToLowerCase(const std::string &amp;input, std::string &amp;output){ output = \"\";}//MyParser::MyParserMyParser::MyParser(){}//MyParser::~MyParserMyParser::~MyParser(){}on obtient un message d’erreur plus conventionnel :[==========] Running 1 test from 1 test suite.[----------] Global test environment set-up.[----------] 1 test from ParserTest[ RUN ] ParserTest.Parser_LowerSingleLetter/.../tests/src/main.cpp:15: FailureExpected equality of these values: output Which is: \"\" \"l\"[ FAILED ] ParserTest.Parser_LowerSingleLetter (0 ms)[----------] 1 test from ParserTest (0 ms total)[----------] Global test environment tear-down[==========] 1 test from 1 test suite ran. (0 ms total)[ PASSED ] 0 tests.[ FAILED ] 1 test, listed below:[ FAILED ] ParserTest.Parser_LowerSingleLetter 1 FAILED TESTEn d’autre termes, le test Parser_LowerSingleLetter échoue car la valeur obtenue par le parser — la chaine de caractère vide — ne correspond pas à la valeur attendue — “l”.Pour que le test soit validé, il faut produire une implémentation correcte de la fonction MyParser::convertToLowerCase dans MyParser.cpp. Pour cela, disons que la fonction parcourt la chaine de caractères, transforme en minuscules tous les caractères et les rajoute dans la variable de sortie. Ce qui nous donne l’implémentation suivante ://MyParser::convertToLowerCasevoid MyParser::convertToLowerCase(const std::string &amp;input, std::string &amp;output){ output = \"\"; for (auto c : input) output.push_back(tolower(c));}Avec cette implémentation, le test est OK. On a produit le code qui valide notre premier test unitaire — je laisse au lecteur le soin de rajouter d’autres tests sur cette première fonction pour éprouver le code s’il le souhaite.[==========] Running 1 test from 1 test suite.[----------] Global test environment set-up.[----------] 1 test from ParserTest[ RUN ] ParserTest.Parser_LowerSingleLetter[ OK ] ParserTest.Parser_LowerSingleLetter (0 ms)[----------] 1 test from ParserTest (0 ms total)[----------] Global test environment tear-down[==========] 1 test from 1 test suite ran. (0 ms total)[ PASSED ] 1 test. La production de code valide le test, on peut avancer dans le développement.Red, Green, RefactorEcrivons notre deuxième test pour la deuxième fonction :TEST(ParserTest, Parser_UpperSingleLetter){ std::string output = \"\"; std::string input = \"l\"; std::string expected = \"L\"; MyParser parser; parser.convertToUpperCase(input, output);\tASSERT_EQ(output, \"L\");}Comme le premier, le deuxième test échoue puisque la fonction MyParser::convertToUpperCase n’est pas définie. Effectuons les modifications de code qui permettent de valider le test.Tout d’abord, la mise à jour des définitions dans le fichier ./toolset/include/MyParser.h,class MyParser{public: MyParser(); ~MyParser(); void convertToLowerCase(const std::string &amp;, std::string &amp;); void convertToUpperCase(const std::string &amp;, std::string &amp;);};puis l’implémentation de la fonction convertToUpperCase dans le fichier ./toolset/src/MyParser.cpp,//MyParser::convertToUpperCasevoid MyParser::convertToUpperCase(const std::string &amp;input, std::string &amp;output){ output = \"\"; for (auto c : input) output.push_back(toupper(c));}L’exécution des tests renvoie maintenant le résultat suivant :[==========] Running 2 tests from 1 test suite.[----------] Global test environment set-up.[----------] 2 tests from ParserTest[ RUN ] ParserTest.Parser_LowerSingleLetter[ OK ] ParserTest.Parser_LowerSingleLetter (0 ms)[ RUN ] ParserTest.Parser_UpperSingleLetter[ OK ] ParserTest.Parser_UpperSingleLetter (0 ms)[----------] 2 tests from ParserTest (0 ms total)[----------] Global test environment tear-down[==========] 2 tests from 1 test suite ran. (0 ms total)[ PASSED ] 2 tests.On voit que le premier test reste valide — ce qui signifie qu’il n’y a pas eu de regression — et que le deuxième test est également OK. La logique de production est toujours la même et peut se résumer en une formule synthétique : Red, Green, Refactor : Red (le test échoue d’abord), Green (le test passe après une modification du code, tous les tests précédents doivent toujours être valides) Refactor (on restructure le code pour une cohérence d’ensemble et on revérifie que tous les tests passent toujours)On remarque le cycle vertueux qui oblige à avancer lentement mais surement en faisant le moins de dégâts possible.Cas d’usage de RefactoringL’approche TDD se prête également bien aux tâches de refactoring pures. Dans ces cas, on profite des tests déjà définis pour controler la qualité du code.Pour notre exemple, essayons un refactoring en deux étapes : Création d’un namespace utils, qui contiendra les définitions de la classe MyParser. Ajout d’une fonction dans la bibliothèque qui renvoie une instance unique de MyParserRefactoring 1 : ajouter un namespacePour la première étape, modifions tout d’abord les tests qui doivent valider le code. La logique des tests n’est pas modifiée — ce qui serait une faute grave — juste l’initialisation du parser.TEST(ParserTest, Parser_LowerSingleLetter){ std::string output = \"\"; std::string input = \"L\"; std::string expected = \"l\"; utils::MyParser parser; // namespace utils parser.convertToLowerCase(input, output);\tASSERT_EQ(output, \"l\");}TEST(ParserTest, Parser_UpperSingleLetter){ std::string output = \"\"; std::string input = \"l\"; std::string expected = \"L\"; utils::MyParser parser; // namespace utils parser.convertToUpperCase(input, output);\tASSERT_EQ(output, \"L\");}Naturellement — et heureusement — ces tests échouent dans un premier temps puisque MyParser.h n’est pas défini dans le bon namespace. Les modifications suivantes vont permettre de les valider.D’abord le fichier ./toolset/include/MyParser.h,#ifndef MYPARSER_H#include &lt;string&gt;namespace utils{ class MyParser { public: MyParser(); ~MyParser(); void convertToLowerCase(const std::string &amp;, std::string &amp;); void convertToUpperCase(const std::string &amp;, std::string &amp;); };} // namespace utils#endif // MYPARSER_HPuis le fichier ./toolset/src/MyParser.cpp, dans lequel il suffira de rajouter la ligne ci-dessous :using namespace utils;Les tests refonctionnent! Refactoring réussi en toute sérénité.Maintenant voyons pour le deuxième refactoring.Refactoring 2: utiliser une référence uniquePour la deuxième étape du refactoring, l’idée est de rajouter une fonction — utils::getParser — qui renvoie une instance unique de MyParser. On testera cette fonction en utilisant une nouvelle classe dans le fichier ./tests/main.cpp ://Les fixtures sont des attributs publiques des classes Testclass UniqueParserTest : public Test{public: utils::MyParser_t &amp;_parser; UniqueParserTest():_parser(utils::getParser()) {}};La classe UniqueParserTest définit une fixture UniqueParserTest::_parser qui est initialisée dans UniqueParserTest::UniqueParserTest().Les tests UniqueParserTest vont permettre de valider, d’une part, que la fonction utils::getParseur retoune toujours la même instance, et d’autre part, que toutes les fonctionnalités du parseur sont bien validées par cette instance.On définit les nouveaux tests suivants — on notera l’utilisation de la macro TEST_F à la place de TEST, ce qui permet exploiter les fixtures dans le test :TEST_F(UniqueParserTest, Parser_UniqueParserIsUnique){ utils::MyParser_t&amp; p = utils::getParser(); ASSERT_EQ(std::addressof(p), std::addressof(_parser));}TEST_F(UniqueParserTest, Parser_UniqueParserLowerSingleLetter){ std::string output = \"\"; std::string input = \"L\"; std::string expected = \"l\"; _parser-&gt;convertToLowerCase(input, output); ASSERT_EQ(output, \"l\");}TEST_F(UniqueParserTest, Parser_UniqueParserUpperSingleLetter){ std::string output = \"\"; std::string input = \"l\"; std::string expected = \"L\"; _parser-&gt;convertToUpperCase(input, output); ASSERT_EQ(output, \"L\");}On notera que le test prévoit l’utilisation du type MyParser_t – à définir dans le code à produire – pour stocker l’instance unique de MyParser.Bonne nouvelle, les tests échouent dans un premier temps, puis (après plusieurs essais), les modifications suivantes permettent de les valider.Tout d’abord le fichier MyParser.h :#ifndef MYPARSER_H#include &lt;string&gt;#include &lt;memory&gt;namespace utils{ class MyParser { public: MyParser(); ~MyParser(); void convertToLowerCase(const std::string &amp;, std::string &amp;); void convertToUpperCase(const std::string &amp;, std::string &amp;); }; typedef std::unique_ptr&lt;MyParser&gt; MyParser_t; MyParser_t&amp; getParser();} // namespace utils#endif // MYPARSER_Hpuis, le fichier MyParser.cpp :#include \"MyParser.h\"using namespace utils;//MyParser::convertToLowerCasevoid MyParser::convertToLowerCase(const std::string &amp;input, std::string &amp;output){ output = \"\"; for (auto c : input) output.push_back(tolower(c));}//MyParser::convertToUpperCasevoid MyParser::convertToUpperCase(const std::string &amp;input, std::string &amp;output){ output = \"\"; for (auto c : input) output.push_back(toupper(c));}//MyParser::MyParserMyParser::MyParser(){}//MyParser::~MyParserMyParser::~MyParser(){}//MyParser_t&amp; utils::getParser()MyParser_t&amp; utils::getParser(){ static MyParser_t p = std::unique_ptr&lt;MyParser&gt;(new MyParser()); return p;}On obtient le résulat suivant après l’exécution des tests:&gt; cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Debug ..## truncated ##build&gt; cmake --build . --config Debug## truncated ##build&gt; ../bin/Debug/toolset_test==========] Running 5 tests from 2 test suites.[----------] Global test environment set-up.[----------] 2 tests from ParserTest[ RUN ] ParserTest.Parser_LowerSingleLetter[ OK ] ParserTest.Parser_LowerSingleLetter (0 ms)[ RUN ] ParserTest.Parser_UpperSingleLetter[ OK ] ParserTest.Parser_UpperSingleLetter (0 ms)[----------] 2 tests from ParserTest (0 ms total)[----------] 3 tests from UniqueParserTest[ RUN ] UniqueParserTest.Parser_UniqueParserIsUnique[ OK ] UniqueParserTest.Parser_UniqueParserIsUnique (0 ms)[ RUN ] UniqueParserTest.Parser_UniqueParserLowerSingleLetter[ OK ] UniqueParserTest.Parser_UniqueParserLowerSingleLetter (0 ms)[ RUN ] UniqueParserTest.Parser_UniqueParserUpperSingleLetter[ OK ] UniqueParserTest.Parser_UniqueParserUpperSingleLetter (0 ms)[----------] 3 tests from UniqueParserTest (0 ms total)[----------] Global test environment tear-down[==========] 5 tests from 2 test suites ran. (0 ms total)[ PASSED ] 5 tests.Le refactoring et l’ajout de la fonction se sont bien déroulés et l’ensemble des tests unitaires définis depuis le début permettent de controler la qualité du code tout au long de la production. Ce qui me permet d’insister sur un aspect intéressant de l’approche TDD : les tests ne sont pas jettables — on peut les faire évoluer, comme tout à l’heure avec l’ajout du namespace, mais ce serait dommage de les supprimer car il permettent de contrôler la qualité du code.ConclusionLe TDD est une approche et pas une technique toute faite. Ce qui signifie qu’il faut s’exercer sur des projets avec rigueur et patience. Plus on s’exerce, plus on a de bons réflexes.Même si les styles de programmation et les langages varient et qu’il est difficile de faire des généralités, on peut quand même définir quelques bonnes pratiques, valables pour tout type de projet. J’en cite 3 : les tests doivent êtres simples et exprimables en langage naturel on doit absolument s’interdire de faire évoluer le code sans avoir défini les tests qui permettront de valider la production il faut diversifier au maximum l’objet des tests (ne pas tester les mêmes choses) — ce qui sera possible en envisageant le maximum de cas d’usage possible. Le lecteur pourra consulter les ouvrages sur le sujet pour se faire une idée plus complète des méthologies TDD.Le code utilisé dans le post est disponible ici. J’attends vos commentaires…Resources CMake.org Git : official Website GitHub.com/google/googletest Crascit.com : cmake-gte docs.microsoft.com : CMake projects in visual studio Wikipedia : Test-driven development Kanmeugne’s Blog : Introduction au TDD (II) – le cycle vertueux (code source)" }, { "title": "Introduction au TDD (I) : Mise en place avec googletest", "url": "/posts/introduction-tdd-googletest/", "categories": "software development", "tags": "cmake, c++, tdd", "date": "2021-10-29 00:00:00 +0800", "snippet": "Le TDD — abbréviation de Test-Driven Development — fait référence à une approche de developpement informatique dans laquelle le code est toujours produit dans le but de valider des tests préalablement définis. Le but de cette approche est de garantir une qualité optimale du code à n’importe quelle étape du développement.Photo by Todd Quackenbush on UnsplashLe MindsetLe développeur qui adopte le TDD suit nécessairement le cycle suivant : Écrire des petits tests S’assurer que les tests échouent dans un premier temps Écrire le code qui permet passer le test S’assurer que le test passe Nettoyer ou réfactorer le code S’assurer que le test passe toujours Retourner à l’étape 1. Le TDD — Test-Driven Development — fait référence à une approche de developpement informatique dans laquelle le code est toujours produit dans le but de valider des tests préalablement définis. Le but de cette approche est de garantir une qualité optimale du code à n’importe quelle étape du développement.Mise en situation : développement de la lib toolsetLe TDD est une approche rigoureuse qui peut être coûteuse à mettre en oeuvre, mais qui apporte beaucoup de sérénité pour les developpeurs sur le long terme. Dans ce post, je propose un petit tutoriel pour s’initier à l’approche TDD. On fera semblant de développer une bibliothèque C++ en utilisant les technologies suivantes : CMake pour le packaging (à installer) googletest pour la gestion des tests unitaires Git pour l’intégration continue et la gestion de version (à installer)La bibliothèque contiendra un parser avec les fonctionnalités suivantes: convertToLowerCase : convertit un mot ou une phrase en minuscule convertToUpperCase : convertit un mot ou une phrase en majusculeLe but de cet exercice est d’appréhender les bonnes pratiques du TDD et de comprendre leur intérêt.Le test zero : il faut que ça compile !La première contrainte que l’on se fixe c’est de pouvoir lancer les commandes ci-dessous, car le test de compilation est le test le plus fondamental! &gt; cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Debug .. &gt; cmake --build . --config Debug &gt; ../bin/Debug/toolset_test Le test de compilation est le test le plus fondamental !Cela suppose de définir les bonnes cibles pour la lib toolset et pour les tests unitaires.Organisation du workspacePour attaquer la production du code qui permettra de valider le test zero, partons de l’arborescence ci-dessous, qui organise le projet en deux sous projets : un pour les tests et un autre pour la lib toolset.project├── CMakeLists.txt # makefile global├── bin # stockage des executables├── lib # stockage des librairies├── build # fichiers de builds├── deps # definitions des dépendances externes├── tests│ ├── CMakeLists.txt # makefile pour les tests│ ├── include│ └── src└── toolset ├── CMakeLists.txt # makefile pour la lib ├── include └── src Le projet s’organise en deux sous projets. Un pour la lib toolset et un autre pour les tests.Les sources pour définir la lib toolset et l’exécutable de tests sont stockées respectivement dans les dossiers toolset/ et tests/. Le makefile global du projet est défini dans ./CMakeLists.txt, à la racine du dossier.Makefile global du projet# CMakeList.txt : Upper level configuration filecmake_minimum_required (VERSION 3.8)# global pathsset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/bin/${CMAKE_BUILD_TYPE}/)set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/lib/${CMAKE_BUILD_TYPE}/)set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/lib/${CMAKE_BUILD_TYPE}/)# project declarationproject (toolset C CXX)# sub projectsadd_subdirectory (\"toolset\")add_subdirectory(\"tests\") Les lignes 4—10 de ./CMakeLists.txt définissent les chemins par défaut pour les cibles et les fichiers intermédiaires, selon les configurations (Release ou Debug), et suivant les architectures. Les lignes 14—16 indiquent que le projet contient deux sous-projets : un pour les tests et l’autre pour la lib toolset.Bien évidemment dans l’état actuel, la compilation échoue puisque les dossiers des sous-projets sont vides. Pour les configurer, on commence par remplir le fichier ./toolset/CMakeLists.txt.Makefile pour la cible toolsetcmake_minimum_required (VERSION 3.8)set(BINARY ${CMAKE_PROJECT_NAME})################################# organize include and src filesset(TOOLSET_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include/)set(TOOLSET_INCLUDE_DIR ${TOOLSET_INCLUDE_DIR} PARENT_SCOPE)file(GLOB_RECURSE TOOLSET_SRC_FILES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)include_directories(${TOOLSET_INCLUDE_DIR})add_library(${BINARY} ${TOOLSET_SRC_FILES}) Le fichier indique que les entêtes se trouvent dans le dossier ./toolset/include/ (ligne 9) et les sources, dans le dossier ./toolset/src/ (lignes 8 et 10). Le nom de la cible est indiqué à la ligne 11 en utilisant la varibale ${CMAKE_PROJECT_NAME}, définie dans le CMakeLists.txt global du projet.Après le makefile de la lib toolset, on remplit le fichier ./tests/CMakeLists.txt pour les tests unitaires. Pour cela, on se sert d’une astuce qui consiste à définir googletest comme une dépendance extérieure et à générer toutes ses cibles au moment de la configuration — la dépendance à googletest est déclarée dans un fichier de configuration intermédiaire, ./deps/gtest/CMakeLists.txt.in.Makefile pour les testscmake_minimum_required (VERSION 3.8)set(BINARY ${CMAKE_PROJECT_NAME}_test)################################## Configure and build GoogleTestconfigure_file( ${CMAKE_SOURCE_DIR}/deps/gtest/CMakeLists.txt.in ${CMAKE_SOURCE_DIR}/build/googletest-download/CMakeLists.txt)execute_process( COMMAND ${CMAKE_COMMAND} -G ${CMAKE_GENERATOR} . RESULT_VARIABLE result WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/build/googletest-download)if(result) message(FATAL_ERROR \"CMake step for googletest failed: ${result}\")endif()execute_process( COMMAND ${CMAKE_COMMAND} --build . RESULT_VARIABLE result WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/build/googletest-download)if(result) message(FATAL_ERROR \"Build step for googletest failed: ${result}\")endif()# Prevent overriding the parent project's compiler/linkerset(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)add_subdirectory( ${CMAKE_SOURCE_DIR}/build/googletest-src ${CMAKE_SOURCE_DIR}/build/googletest-build)################################## organize include and src filesset( GTEST_INCLUDE_DIR ${CMAKE_SOURCE_DIR}/build/googletest-src/googlemock/include/ ${CMAKE_SOURCE_DIR}/build/googletest-src/googletest/include/)set( TEST_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include/)file( GLOB_RECURSE ${TEST_SRC_FILES} ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)include_directories( ${GTEST_INCLUDE_DIR} ${TEST_INCLUDE_DIR} ${TOOLSET_INCLUDE_DIR})add_executable (${BINARY} ${TEST_SRC_FILES})target_link_libraries(${BINARY} ${CMAKE_PROJECT_NAME} gmock_main) le makefile des tests fait appel à une dépendance extérieure (ligne 5) pour générer les entêtes et les cibles de googletest à la configuration (lignes 5—30). La cible pour l’exécutable des tests est complètement définie de la ligne 51 — 52.Le fichier ./tests/CMakeLists.txt.in utilisé pour la dépendence à googletest indique le lien github officiel des sources et le tag à utiliser.cmake_minimum_required (VERSION 3.8)project(googletest-download NONE)include(ExternalProject)ExternalProject_Add(googletest GIT_REPOSITORY https://github.com/google/googletest GIT_TAG release-1.10.0 SOURCE_DIR \"${CMAKE_SOURCE_DIR}/build/googletest-src\" BINARY_DIR \"${CMAKE_SOURCE_DIR}/build/googletest-build\" CONFIGURE_COMMAND \"\" BUILD_COMMAND \"\" INSTALL_COMMAND \"\" TEST_COMMAND \"\") Le fichier ./deps/gtest/CMakeLists.txt.in est utilisé dans ./tests/CMakeLists.txt au moment de la configuration et de la création des cibles googletest. Il indique le lien github pour récupérer les sources et le tag à utiliser.Code source TDD–compatibleDans l’état actuel, la production de code ne valide toujours pas le test zero . Notamment, la commande :&gt; cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Debug ..renvoie un code d’erreur car les fichiers sources pour les sous projets tests et toolset sont inexistants! On complète l’initialisation du projet avec le strict minimum pour pouvoir valider la compilation.Premièrement, le header ./toolset/include/MyParser.h pour déclarer le parser :#ifndef MYPARSER_Hclass MyParser;#endif // MYPARSER_HEnsuite, le fichier source ./toolset/src/MyParser.cpp, qui ne contient qu’une ligne pour l’instant :#include \"MyParser.h\"Enfin, le fichier source./tests/src/main.cpp pour la définition et l’exécution des tests :#include &lt;gmock/gmock.h&gt;using namespace testing;int main(int argc, char** argv){ testing::InitGoogleMock(&amp;argc, argv); return RUN_ALL_TESTS();}Validation du test zeroLa production de code fournie ci-dessus valide notre test zero ! La commande cd build &amp;&amp; cmake -DCMAKE_BUILD_TYPE=Debug .. génère un makefile système (ou fichier .sln sous windows avec visual studio), et la commande cmake --build . --config Debug génère les bonnes cibles comme on peut le voir sur l’arborescence ci-dessous..├── bin│ └── Debug│ └── toolset_test├── CMakeLists.txt├── deps│ └── gtest│ └── CMakeLists.txt.in├── lib│ └── Debug│ └── libtoolset.a├── tests│ ├── CMakeLists.txt│ └── src│ └── main.cpp└── toolset ├── CMakeLists.txt ├── include │ └── MyParser.h └── src └── MyParser.cpp Résultat de la commande cmake --build . --config Debug sur une machine ubuntu avec gcc-7.5. La lib toolset — ./lib/Debug/libtoolset.a — est correctement générée ainsi que l’exécutable pour les tests — ./bin/Debug/toolset.Pour l’instant lorsqu’on lance l’exécutable de tests on obtient un message qui nous indique qu’aucun test n’a été défini — ce qui est tout à fait normal. La suite de l’exercice consiste à définir les tests unitaires qui permettront de valider chacune des fonctionnalités de la lib toolset à terme.&gt; ./bin/Debug/toolset[==========] Running 0 tests from 0 test suites.[==========] 0 tests from 0 test suites ran. (0 ms total)[ PASSED ] 0 tests. Les sources sont disponibles sur github, pour les impatients.Maintenant que la mise en place de l’espace de travail est faite, on peut véritablement entrer dans le cycle vertueux des TDD.ConclusionLe plus dur du travail est fait avec cette mise en place. C’est très important de valider le test zero car c’est la condition nécessaire pour travailler itérativement par la suite.Dans la deuxième partie de cet exercice, on va se concentrer sur l’implémentation des fonctionnalités de la lib. On verra que l’approche TDD eprmet d’envisager sereinement l’évolution du code et le refactoring.Resources CMake.org Git : official Website GitHub.com/google/googletest Crascit.com : cmake-gte docs.microsoft.com : Cmake projects in visual studio Wikipedia : Test-driven development Kanmeugne’s Blog : Introduction au TDD (I) – Mise en place avec googletest (code source) Kanmeugne’s Blog : Introduction aux TDD (II) – Le cycle vertueux" }, { "title": "Pheromon evaporation on a 2D Grid", "url": "/posts/pheromons-evaporation/", "categories": "modeling & simulation", "tags": "sfml, cmake, c++, modeling, pheromon, evaporation, simulation", "date": "2020-10-12 00:00:00 +0800", "snippet": "You should now be familiar with our 2D Grid app. In a previous post, I updated its original object-oriented architecture in order to implement an affordable obstacle feature. Here, I am going to upgrade the architecture again in order to implement a pheromon evaporation feature.Photo by Danny Howe on UnsplashYou can see pheromon like a chemical substance that is dropped somewhere — as a mark of an organic activity — and then evaporates overtime. Pheromon paradigm is a very productive way to implement a stigmergic behavior — which is a type of behavior based on indirect communication through a common and shared space. Stigmergy explains the emergence of collective behavior among several social species with limited intellectual abilities. The concept has been first introduced by the french biologist Pierre-Paul Grassé and systematically studied by Deneubourg for different ants species.The proposed improvements of the previous object-oriented architecture will focus on mimicking and illustrating an ant-like pheromon evaporation. Simply put, I am going to do the following : add another viewer for pheromon — PheromonViewer — and more controls in the App object – App::addPheromon upgrade IGrid, and consequently Grid, to declare and implement pheromon related methods define a new method — App::evaporate — responsible of the evaporation process. Fig. 1. Architecture of our 2D Grid Appsfml2dgrid.├── CMakeLists.txt├── deps│ └── sfml│ └── CMakeLists.txt.in└── sfml2dgrid ├── CMakeLists.txt ├── main.cpp ├── app │   ├── include │   │   ├── App.h │   │   └── dynamics.h │   └── src │   └── App.cpp ├── env │   ├── include │   │   ├── Grid.h │   │   └── IGrid.h │   └── src │   └── Grid.cpp ├── geometry │   ├── include │   │   └── geometry.h │   └── src └── viewers ├── include │   ├── AbstractViewer.h │   ├── GridViewer.h │   ├── ObstacleViewer.h │   └── PheromonViewer.h └── src ├── AbstractViewer.cpp ├── GridViewer.cpp ├── ObstacleViewer.cpp └── PheromonViewer.cpp The file tree of the project with the source (.cpp) and header (.h) files. I am just going to discuss about the upgrade that I made from the previous version.Pheromon modeling and evaporationThe App object is augmented with App::addPheromon and App::evaporate methods both responsible of adding a little amount of pheromon in a selected cell, and evaporating pheromons over time — see Fig. 2. Fig. 2. App and IGrid improvementsApp::evaporate will take a time interval as parameter in order to schedule the evaporation process — I use SFML clocks to implement this.App.h#ifndef APP_H#define APP_Hnamespace sf{\tclass RenderWindow;};namespace env{\tclass IGrid;};namespace viewers{\tclass AbstractViewer;};class App{private:\t// sfml render window\tsf::RenderWindow *_window = nullptr;\t// the 2D grid pointer\tenv::IGrid *_grid = nullptr;\t// a pointer to the viewer\t// this could be a set of viewer actually\t// if we consider component behavior\tviewers::AbstractViewer *_viewer = nullptr;public:\t// theorical width of the environment\t// will match the grid width in terms of number of cells.\tstatic const int DEFAULT_WIDTH;\t// theorical height of the environment.\tstatic const int DEFAULT_HEIGHT;\t// x-resolution of the grid i.e. the x-size of a cell\tstatic const int DEFAULT_RESX;\t// y-resolution of the grid i.e. the y-size of a cell\tstatic const int DEFAULT_RESY;\t// attach window to the app\tvoid setWindow(sf::RenderWindow *);\t// attach a specific viewer\tvoid setViewer(viewers::AbstractViewer *);\t// attach a grid (should have been initialized)\tvoid setGrid(env::IGrid *);\t// return the attached grid\tenv::IGrid *getGrid();\t// return the attached window\tsf::RenderWindow *getWindow();\t// run the application (the logic)\tvoid run();\t// show content (display routines)\tvoid display();\t// evaporation cycle\tvoid evaporate();\t// add obstacle control\tbool addObstacle(int, int);\t// remove obstacle control\tbool removeObstacle(int, int);\t// add Pheromon\tbool addPheromon(int, int);\tApp() = default;\tvirtual ~App();};#endif // !APP_HA special method to apply evaporation is also defined in IGrid — IGrid::iUpdatePheromon.IGrid.h#ifndef IGRID_H#define IGRID_Hnamespace env{ struct CELL { int _id; // id of the cell bool _mask; float _tau; // amount of pheromon CELL() = default; CELL(const CELL &amp;) = default; }; // Functor definition to apply on cell // We can inherit from this to function // to apply on cells class ICellFunctor { public: virtual void operator()( const CELL &amp; // cell_id ) = 0; }; // IGrid class IGrid { public: virtual ~IGrid() = default; // returns the width virtual int iGetSizeX() const = 0; // returns the height virtual int iGetSizeY() const = 0; // returns the number of cells in the grid virtual int iGetNumberOfCells() const = 0; // gets the width of a cell (in terms of pixels) virtual int iGetResolutionX() const = 0; // gets the height of a cell (in terms of pixels) virtual int iGetResolutionY() const = 0; // applies functor on Cells virtual void iApplyOnCells(ICellFunctor &amp;) const = 0; //-- Test // relative position of a cell according to its id virtual bool iGetCellPosition( const CELL &amp;, // cell int &amp;, // posx int &amp; // posy ) const = 0; // coordinates of a cell accoring to its id virtual bool iGetCellCoordinates( const CELL &amp;, // cell int &amp;, // row_number int &amp; // column_number ) const = 0; // cell rank of the the cell according // to its relative position in the grid virtual bool iGetCellNumber( int, // row_number int, // column_number CELL &amp;) const = 0; // the containing cell given the coordinates in the 2D space virtual bool iGetContainingCell( int, // posx int, // posy CELL &amp; // cell ) const = 0; // checks if a given point is within a given cell virtual bool iIsWithinCell( int, // posx int, // posy const CELL &amp; // cell ) const = 0; // initializes the vector of cells, obstacle mask, etc. virtual void iInitialize() = 0; // add obstacle to the grid virtual bool iAddObstacle(const CELL &amp;) = 0; // remove obstacle from the grid virtual bool iRemoveObstacle(const CELL &amp;) = 0; // return the obstacle status : true if obstacle, false otherwise virtual bool iIsObstacle(const CELL &amp;) const = 0; // pheromons virtual bool iAddPheromon( const CELL &amp;,// cell no const float // pheromon deposit ) = 0; virtual void iUpdatePheromon(const int&amp;) = 0; };} // namespace env#endif // !IGRID_HBasically, IGrid::iUpdatePheromon will update the amount of pheromons for each cell — CELL::tau — by running the following formula:\\[\\tau_{ij}^{t} = \\tau_{ij}^{t-1} \\cdot (1 - \\rho) + \\Delta^{t}\\tau_{ij}\\]Where : \\(\\tau_{ij}^{t}\\) is the amount of pheromons in \\(cell_{ij}\\) in the current timestep \\(\\tau_{ij}^{t-1}\\) is the amount of pheromons in \\(cell_{ij}\\) in the previous timestep \\(\\Delta^{t}\\tau_{ij}\\) is the amount of pheromon injected in the current timestep \\(\\rho \\in [0,1]\\) is the evaporation coefficientAs you might have guessed, this method will be called in App::evaporate at specific time intervals.ParametersFor numerical robustness, I will use the following global parameters in the implementation: \\(P_{max}\\) : the pheromon maximum capacity of a cell \\(P_{min}\\) : the pheromon minimal capacity of a cell (below this value, the amount of pheromon is set to \\(0\\))The interested reader can refer to the source code to check/set the value for parameters : \\(\\rho, P_{min}\\) and \\(P_{max}\\)PheromonViewerTo visualize pheromons and especially the evaporation process, I added a PheromonViewer that will be called in the App::display method. PheromonViewer::iDraw applies an ICellFunctor on every cell of the grid — if their corresponding amount of pheromon is greater than zero — that draws a red mark on the screen according to their current state.Fig. 3.ViewerMgr is a meta viewer that agregates more than one viewer. It will be used to add a viewer for pheromon (PheromonViewer) next to the viewers forlines (GridViewer) and obstacles (ObstacleViewer) without changing the relationship between App and AbstractViewerPheromonViewer.h#ifndef PHEROMONVIEWER_H#define PHEROMONVIEWER_H#include \"AbstractViewer.h\"namespace env{ class ICellFunctor;};namespace viewers{ class PheromonViewer : public AbstractViewer { public: PheromonViewer() = default; virtual ~PheromonViewer() = default; protected: virtual void iDraw(); private: void drawPheromon(env::ICellFunctor &amp;); };} // namespace viewers#endif // !PHEROMONVIEWER_HDemoWe are ready to instanciate our brand new App object with all the improvements for pheromon manipulation.main.cpp#include \"App.h\"#include \"GridViewer.h\"#include \"ObstacleViewer.h\"#include \"PheromonViewer.h\"#include \"Grid.h\"#include &lt;thread&gt;#include &lt;SFML/Graphics.hpp&gt;#ifdef __linux__#include &lt;X11/Xlib.h&gt;#endifint main(){#ifdef __linux__ XInitThreads();#endif // -- sfml windows sf::ContextSettings settings; settings.antialiasingLevel = 10; sf::RenderWindow window( sf::VideoMode( (App::DEFAULT_WIDTH*App::DEFAULT_RESX), (App::DEFAULT_HEIGHT*App::DEFAULT_RESY) ), \"SFML 2D Grid\", sf::Style::Titlebar | sf::Style::Close, settings ); window.clear(sf::Color::White); window.setFramerateLimit(120); window.setActive(false); // -- application App app; app.setWindow(&amp;window); //-- grid 2D env::Grid g; g.setSizeX(App::DEFAULT_WIDTH); g.setSizeY(App::DEFAULT_HEIGHT); g.setResolutionX(App::DEFAULT_RESX); g.setResolutionY(App::DEFAULT_RESY); g.iInitialize(); app.setGrid(&amp;g); //-- viewer viewers::GridViewer gviewer; app.setViewer(&amp;gviewer); gviewer.initialize(); gviewer.iActivate(); // grid obstacles viewers::ObstacleViewer oviewer; oviewer.iActivate(); // pheromons viewers::PheromonViewer pviewer; pviewer.iActivate(); // aggregator viewers::ViewerMgr mgr; mgr.iAddViewer(&amp;oviewer); mgr.iAddViewer(&amp;pviewer); mgr.iAddViewer(&amp;gviewer); app.setViewer(&amp;mgr); mgr.iActivate(); // initialize gviewer (only after having attached it to the App object) gviewer.initialize(); //-- launch application std::thread rendering_thread(&amp;App::display, &amp;app); std::thread evaporation_thread(&amp;App::evaporate, &amp;app); app.run(); rendering_thread.join(); evaporation_thread.join(); return 0;}The interested reader can fork the complete source code from here and run the following in a terminal at the project folder root : # on windows $ cmake -G \"Visual Studio $(Version)\" -S . -B ./build $ cmake --build ./build --config Debug --target app $ ./bin/Debug/app # on linux $ mkdir build $ cd build $ cmake -G \"Unix Makefiles\" .. -DCMAKE_BUILD_TYPE=Debug $ cmake --build ./ --target app $ ../bin/Debug/appThe program should display a clickable 2D Grid where the right-click adds an obstacle on the selected cell and the left-click removes it. With the mouse middle you should be able to drop pheromon on the grid. Once pheromons are dropped they automatically and smoothly start to evaporate.Step by step demo : launching the Pheromon Evaporation SFML App from the terminal (built on ubuntu 18.08 with gcc 7.5)Enjoy and feel free to send me your feedbacks!References SFML::Clock fr.wikipedia.org/wiki/Stigmergie The self-organizing exploratory pattern of the argentine ant - Deneubourg J, Aron S, Goss S et al. kanmeugne/sfml2dgrid : sfml-2d-obstacles-pheromons Kanmeugne’s Blog : Drawing a 2D Grid with SFML Kanmeugne’s Blog : 2D Grid with obstacles" }, { "title": "Sur l'importance de l'approche multi-agents pour les développeurs", "url": "/posts/masmatters/", "categories": "multi-agent systems", "tags": "sma, stigmergy, mas", "date": "2020-10-08 00:00:00 +0800", "snippet": "Le développement informatique a connu d’importantes évolutions ces dernières années.Pour être toujours plus proche des besoins — accélérer la prise de décision dans les entreprises et les institutions publiques, optimiser les temps de calcul et de de déploiement de nouvelles solutions — l’industrie de l’informatique multiplie les moyens techniques et les approches, faisant évoluer le métier de développeur informatique et le rendant de plus en plus complexe.Photo by Abraham Barrera on UnsplashParmi les différentes tendances qui déterminent l’évolution du métier de développeur informatique, on peut distinguer : La multiplication des périphériques d’acquisition, de traitement et de restitution de la donnée. Ces périphériques ou accessoires sont des ordinateurs à part entière — car dotés de processeurs et programmables. De par leur petite taille et leur omniprésence dans l’espace vital des utilisateurs, ces objets permettent de diversifier l’offre de services informatiques. Le développement des technologies de mise en réseau. Internet a complètement changé le rapport à l’ordinateur pour un développeur informatique. L’ordinateur ne peut plus être vu comme une ressource de calcul isolée et les réseaux informatiques sont devenus la norme. La possibilité de monter plusieurs centaines de milliers d’ordinateurs en réseau et tirer ainsi profit de la somme des ressources de calcul pour l’exécution des programmes informatiques incite à repenser la manière de les concevoir. L’évolution de la complexité des solutions informatiques. Pour des raisons à la fois économiques et techniques, il devient de plus en plus difficile de penser une application sans l’intégrer dans un système d’interactions avec des serveurs de base de données ou d’applications tierces. En effet, plusieurs fournisseurs d’applications se spécialisent dans des tâches bien précises accessibles via des interfaces d’utilisation rendues publiques. Par ailleurs, il n’est pas toujours possible matériellement, de mettre en place des ressources informatiques réunissant toutes les caractéristiques nécessaires au bon fonctionnement d’une application (capacité de stockage, vitesse d’exécution, communication réseau, etc.), d’où le besoin de les spécialiser. Le besoin d’automatisation du traitement de l’information et la robotisation massive du travail manuel. L’informatique fait de plus en plus de choses pour nous, et ce, sans notre intervention. Des applications M2M (Machine to Machine) se développent de plus en plus, promues par les besoins des utilisateurs et la transformation digitale dans les entreprises et les espaces publiques. Les solutions informatiques se veulent plus pro-actives et embarquées. Le facteur humain. L’évolution des approches de la programmation et des attentes des utilisateurs est en grande partie due à une relation de plus en plus étroite entre la machine et l’homme. En effet, les utilisateurs ont tendance à transposer leurs facultés intellectuelles et émotives sur la machine et s’attendent à ce qu’elle parle le même langage qu’eux. Les approches du développement deviennent par conséquent de plus en plus abstraites — haut niveau — quand il s’agit de créer des outils informatiques capable d’interagir avec les utilisateurs dans leurs tâches quotidiennes. Aujourd’huiLe croisement de ces différentes tendances entraîne un changement de perspectives important pour le métier de développeur informatique. L’automatisation, la robotisation et l’évolution de la complexité des solutions informatiques impliquent de savoir mettre en œuvre des programmes informatiques capables de s’exécuter de manière autonome et d’interagir avec d’autres programmes en tenant compte des intérêts des utilisateurs objectivement exprimés. Les réseaux informatiques et la distribution des ressources, étant devenus la norme, la décomposition de l’exécution d’une application en sous programmes et la mise en œuvre des interactions entre ces différents programmes sont des prérequis incontournables pour le déploiement réussi d’une application. Prendre les intérêts d’un utilisateur ne signifie plus simplement réaliser des programmes qui produisent un résultat, mais implique dorénavant la mise en place de formes de coopération entre programmes, de compétition ou de négociation — c’est donc une ère très sociale du développement où la distribution, les interactions et leur complexité se confondent avec les fonctionnalités des solutions informatiques ! Pourquoi les systèmes multi-agents ?L’ingénieur informatique doit dont penser système et pas n’importe quel système! L’ingénieur doit penser systèmes multi-agents (SMA), c’est-à-dire, des programmes situés dans des machines et interagissant entre eux pour réaliser une tâche. Il pourra profiter d’une abondante littérature sur les actes de langages, les architectures de décision des agents rationnels, des agents réactifs, ou encore, des modèles d’action inspirés de la robotique et adaptés à un environnement dynamique. Les SMA offrent au modélisateur informatique une plus grande liberté de conception pour penser les solutions aux problèmes qui lui sont posés et une manière naturelle d’intégrer les ressources de calculs et de stockage dès la conception.Un autre avantage, et non des moindres, est le parallèle extrêmement enrichissant que l’on peut faire entre les ecosystèmes naturels et l’architecture de solutions informatiques. La dimension réseau étant devenue la norme, comme mentionné plus haut, il est possible de s’inspirer de la nature (fourmis, araignées, bactéries, plantes, etc. rechercher nature-inspired algorithms) pour imaginer des modèles d’interactions efficaces entre les programmes.ConclusionVoilà en quelques mots ce qui me motive à mettre en avant l’approche multi-agent pour les développeurs informatique. Bien entendu, il faudra apporter des preuves par la pratique. C’est pourquoi, très prochainement, je reviendrai en détails et plus concrètement sur l’intérêt du paradigme SMA pour : le développement web l’apprentissage automatique et l’optimisation la simulation comportementaleD’ici-là, merci de vos retours." }, { "title": "2D Grid with obstacles", "url": "/posts/2d-grid-obstacles/", "categories": "modeling & simulation", "tags": "sfml, cmake, c++, simulation, modeling", "date": "2020-10-04 00:00:00 +0800", "snippet": "Using a regular 2D Grid to model the navigable space is a good choice if you want to simulate moving agents (ex: vehicules, pedestrians). In fact, 2D Grids can be seen as partitions of the space and, therefore, provide an excellent framework for path planning and collision avoidance algorithms deployment. Moreover, by adding state variables to grid cells, we end up with a very affordable way to manage obstacles and other kind of semantics in the space.Photo by Iewek Gnos on UnsplashIn this post, I am upgrading an existing object oriented architecture that I shared recently as a starting point for those who wanted to have a 2D Grid in their simulation app. Back then, the provided features were limited to grid dimension setting and visualization. In this new version, I am adding a simple obstacle management by attaching state variables to grid cells — a complete implementation in C++ is also provided for demonstration. Fig. 1. Architecture of our 2D Grid Appsfml2dgrid.├── CMakeLists.txt├── deps│ └── sfml│ └── CMakeLists.txt.in└── sfml2dgrid ├── CMakeLists.txt ├── main.cpp ├── app │ ├── include │ │ └── App.h │ └── src │ └── App.cpp ├── env │ ├── include │ │ ├── Grid.h │ │ └── IGrid.h │ └── src │ └── Grid.cpp ├── geometry │ └── include │ └── geometry.h └── viewers ├── include │ ├── AbstractViewer.h │ ├── GridViewer.h │ └── ObstacleViewer.h └── src ├── AbstractViewer.cpp ├── GridViewer.cpp └── ObstacleViewer.cpp The file tree of the project with the source (.cpp) and header (.h) files. I am just going to discuss about the upgrade that I made from the previous version.Comparing to the previous version, I have updated 3 existing objects — App, env::IGrid and env::Grid — and created 3 new objects — viewers::ObstacleViewer, env::ICellFunctor and viewers::ViewerMgr. More details below.AppThe App object is augmented with App::addObstacle and App::removeObstacle both responsible of adding and removing obstacles in the 2D Grid respectively (see Fig. 2). As I teased in the introdution, state variables are associated to grid cells in order to store occupancy information — this is how the model handle obstacles : if a cell occupied, it is considered as an obstacle. Fig. 2. App Object (with addObstacle and removeObstacleApp.h#ifndef APP_H#define APP_Hnamespace sf{\tclass RenderWindow;};namespace env{\tclass IGrid;};namespace viewers{\tclass AbstractViewer;};class App{private:\t// sfml render window\tsf::RenderWindow *_window = nullptr;\t// the 2D grid pointer\tenv::IGrid *_grid = nullptr;\t// a pointer to the viewer\t// this could be a set of viewer actually\t// if we consider component behavior\tviewers::AbstractViewer *_viewer = nullptr;public:\t// theorical width of the environment\t// will match the grid width in terms of number of cells.\tstatic const int DEFAULT_WIDTH;\t// theorical height of the environment.\tstatic const int DEFAULT_HEIGHT;\t// x-resolution of the grid i.e. the x-size of a cell\tstatic const int DEFAULT_RESX;\t// y-resolution of the grid i.e. the y-size of a cell\tstatic const int DEFAULT_RESY;\t// attach window to the app\tvoid setWindow(sf::RenderWindow *);\t// attach a specific viewer\tvoid setViewer(viewers::AbstractViewer *);\t// attach a grid (should have been initialized)\tvoid setGrid(env::IGrid *);\t// return the attached grid\tenv::IGrid *getGrid();\t// return the attached window\tsf::RenderWindow *getWindow();\t// run the application (the logic)\tvoid run();\t// show content (display routines)\tvoid display();\t// add obstacle control\tbool addObstacle(int, int);\t// remove obstacle control\tbool removeObstacle (int, int);\tApp() = default;\tvirtual ~App();};#endif // !APP_HIGridThe IGrid interface is augmented with 3 obstacle-related methods — IGrid::iAddObstacle, IGrid::iRemoveObstacle and IGrid::iIsObstacle — necessary to edit the state of a CELL status (IGrid::iAddObstacle and IGrid::iRemoveObstacle) and to check whether a given CELL is an obstacle or not (IGrid::iIsObstacle).IGrid defines one more method called IGrid::iApplyOnCells which takes a functor on cells — env::ICellFunctor — as the only parameter and applies it on every cell of the grid. For the record, this method is called in ObstacleViewer::drawObstacles method (see next section), in charge of displaying the obstacles of the grid. Fig. 3 gives extensive details about the IGrid new look and its relations with other classes definitions. Fig. 3. Evolution of the IGrid interface, with 4 more methods :iAddObstacle, iRemoveObstacle, iApplyOnCells and iIsObstacleIGrid.h#ifndef IGRID_H#define IGRID_Hnamespace env{ struct CELL { int _id; // id of the cell bool _mask; CELL() = default; CELL(const CELL &amp;) = default; }; // Functor definition to apply on cell // We can inherit from this to function // to apply on cells class ICellFunctor { public: virtual void operator()( const CELL &amp; // cell_id ) = 0; }; // IGrid class IGrid { public: virtual ~IGrid() = default; // returns the width virtual int iGetSizeX() const = 0; // returns the height virtual int iGetSizeY() const = 0; // returns the number of cells in the grid virtual int iGetNumberOfCells() const = 0; // gets the width of a cell (in terms of pixels) virtual int iGetResolutionX() const = 0; // gets the height of a cell (in terms of pixels) virtual int iGetResolutionY() const = 0; // applies functor on Cells virtual void iApplyOnCells(ICellFunctor &amp;) const = 0; //-- Test // relative position of a cell according to its id virtual bool iGetCellPosition( const CELL &amp;, // cell int &amp;, // posx int &amp; // posy ) const = 0; // coordinates of a cell accoring to its id virtual bool iGetCellCoordinates( const CELL &amp;, // cell int &amp;, // row_number int &amp; // column_number ) const = 0; // cell rank of the the cell according // to its relative position in the grid virtual bool iGetCellNumber( int, // row_number int, // column_number CELL &amp;) const = 0; // the containing cell given the coordinates in the 2D space virtual bool iGetContainingCell( int, // posx int, // posy CELL &amp; // cell ) const = 0; // checks if a given point is within a given cell virtual bool iIsWithinCell( int, // posx int, // posy const CELL &amp; // cell ) const = 0; // initializes the vector of cells, obstacle mask, etc. virtual void iInitialize() = 0; // add obstacle to the grid virtual bool iAddObstacle(const CELL &amp;) = 0; // remove obstacle from the grid virtual bool iRemoveObstacle(const CELL &amp;) = 0; // return the obstacle status : true if obstacle, false otherwise virtual bool iIsObstacle(const CELL &amp;) const = 0; };} // namespace env#endif // !IGRID_HViewerMgr and ObstacleViewerViewerMgr is a special AbstractViewer that agregates (cf. Composite pattern) other AbstractViewer with the method ViewerMgr::iAddViewer. I introduce this pattern in order to separate view concerns and to be able to activate several views at the same time. We will use this meta viewer to attach an ObstacleViewer to the App in order to display the grid lines and the obstacles at the same time. Fig. 4. ViewerMgr is a meta viewer that agregates more than one viewer. It will be used to add a viewer for obstacle (ObstacleViewer) next to the viewer forlines (GridViewer) without changing the relationship between App and AbstractViewerAbstractViewer.h#ifndef ABSTRACTVIEWER_H#define ABSTRACTVIEWER_H#include &lt;vector&gt;class App;namespace viewers{\t// AbstractViewer\tclass AbstractViewer\t{\tpublic:\t\t// activate the viewer. If activated, it provides the desired view\t\tvirtual void iActivate();\t\t// deactivate the viewer. Do not display anaything if deactivated\t\tvirtual void iDeactivate();\t\t// return True if the viewer is active\t\tvirtual bool iIsActive() const;\t\t// display function\t\tvirtual void iDisplay();\t\t// attach the application object\t\tvirtual void iSetApp(App *);\t\tvirtual ~AbstractViewer() = default;\t\tAbstractViewer() = default;\tprotected:\t\t// specific draw method (to be concretized in child classes)\t\tvirtual void iDraw() = 0;\t\tbool _active = false;\t\tApp *_app;\t};\t// viewer manager, using the composite pattern to\t// aggregate several viewers into one\tclass ViewerMgr : public AbstractViewer\t{\tpublic:\t\tvirtual void iAddViewer(AbstractViewer *);\t\tvirtual ~ViewerMgr() = default;\t\tViewerMgr() = default;\t\tvirtual void iSetApp(App *) override;\tprotected:\t\tvirtual void iDraw();\tprivate:\t\tstd::vector&lt;AbstractViewer *&gt; _viewers;\t};} // namespace viewers#endif // ABSTRACTVIEWER_HObstacleViewer.h#ifndef OBSTACLEVIEWER_H#define OBSTACLEVIEWER_H#include \"AbstractViewer.h\"namespace env{ class ICellFunctor;};namespace viewers{ class ObstacleViewer : public AbstractViewer { public: ObstacleViewer() = default; virtual ~ObstacleViewer() = default; protected: virtual void iDraw(); private: void drawObstacles(env::ICellFunctor &amp;); };} // namespace viewers#endif // !OBSTACLEVIEWER_HDemoWe are ready to run our brand new App with all the improvements. The main.cpp for the 2D grid demo App with obstacle management is exactly the following :#include \"App.h\"#include \"GridViewer.h\"#include \"ObstacleViewer.h\"#include \"Grid.h\"#include &lt;thread&gt;#include &lt;SFML/Graphics.hpp&gt;#ifdef __linux__#include &lt;X11/Xlib.h&gt;#endifint main(){#ifdef __linux__ XInitThreads();#endif // -- sfml windows sf::ContextSettings settings; settings.antialiasingLevel = 10; sf::RenderWindow window( sf::VideoMode( (App::DEFAULT_WIDTH*App::DEFAULT_RESX), (App::DEFAULT_HEIGHT*App::DEFAULT_RESY) ), \"SFML 2D Grid with obstacles\", sf::Style::Titlebar | sf::Style::Close, settings ); window.clear(sf::Color::White); window.setFramerateLimit(120); window.setActive(false); // -- application App app; app.setWindow(&amp;window); //-- grid 2D env::Grid g; g.setSizeX(App::DEFAULT_WIDTH); g.setSizeY(App::DEFAULT_HEIGHT); g.setResolutionX(App::DEFAULT_RESX); g.setResolutionY(App::DEFAULT_RESY); g.iInitialize(); app.setGrid(&amp;g); //-- viewer viewers::GridViewer gviewer; gviewer.iActivate(); // grid obstacles viewers::ObstacleViewer oviewer; oviewer.iActivate(); // aggregator viewers::ViewerMgr mgr; mgr.iAddViewer(&amp;oviewer); mgr.iAddViewer(&amp;gviewer); app.setViewer(&amp;mgr); mgr.iActivate(); // initialize gviewer (only after having attached it to the App object) gviewer.initialize(); //-- launch application std::thread rendering_thread(&amp;App::display, &amp;app); app.run(); rendering_thread.join(); return 0;}The interested reader can fork the complete source code from here and run the following in a terminal at the root of the project folder :On windows $ cmake -G \"Visual Studio $(Version)\" -S . -B ./build $ cmake --build ./build --config Debug --target app $ ./bin/Debug/appOn linux $ mkdir build $ cd build $ cmake -G \"Unix Makefiles\" .. -DCMAKE_BUILD_TYPE=Debug $ cmake --build ./ --target app $ ../bin/Debug/appThe program should display a clickable 2D Grid where the right-click adds an obstacle on the selected cell and the left-click removes it.Step by step demo : launching the 2D Grid with obstacles SFML App from the terminal (built on ubuntu 18.08 with gcc 7.5)Enjoy and feel free to send me your feedbacks!References kanmeugne/sfml2dgrid : sfml-2d-obstacles-grid Kanmeugne’s Blog : Kanmeugne’s Blog : Drawing a 2D Grid with SFML" }, { "title": "Drawing a 2D Grid with SFML", "url": "/posts/sfml-2d-grid/", "categories": "modeling & simulation", "tags": "sfml, cmake, c++, simulation, modeling", "date": "2020-09-29 00:00:00 +0800", "snippet": "I started studying simulation of moving agents ten years ago and I have come to realize that regular 2D Grids are extraordinary abstractions for the navigable space. In fact, regular 2D grids are very easy to encode and they offer an elegant framework for path planning and collision avoidance algorithms deployment.Photo by Glenn Carstens-Peters on UnsplashIn this post, I am sharing an object oriented architecture — with its C++ implementation — that I am actually using in my personal projects when I need a 2D Grid. I hope that it could be an affordable starting point for anyone who is interested in the subject.ArchitectureTo be very concrete, I am going to assume that I am building a simulation app — of moving agents — that uses a 2D grid as the navigable space model. The root object oriented architecture that I am using to tackle the implementation is outlined in Fig. 1. Basically, there is an application object called App and three other packages: env : for the navigable space representation and manipulation objects viewers : for visualisation purposes geometry : for geometrical abstractionI am giving more details about App, env, viewers and geometry below. Fig. 1. Architecture of our 2D Grid Appsfml2dgrid.├── CMakeLists.txt├── deps│ └── sfml│ └── CMakeLists.txt.in└── sfml2dgrid ├── CMakeLists.txt ├── main.cpp ├── app │ ├── include │ │ └── App.h │ └── src │ └── App.cpp ├── env │ ├── include │ │ ├── Grid.h │ │ └── IGrid.h │ └── src │ └── Grid.cpp ├── geometry │ └── include │ └── geometry.h └── viewers ├── include │ ├── AbstractViewer.h │ └── GridViewer.h └── src ├── AbstractViewer.cpp └── GridViewer.cpp The file tree of the project with the source (.cpp) and header (.h) files that I am going to discuss about in the rest of the postApp : the Application ObjectThe App object is the definition of our application — see Fig.2.It holds a viewer (viewers::AbstractViewer) — that contains display instructions — and a reference to a 2D Grid (env::IGrid) — that will be used to manipulate the 2D grid. Note that the App object has an SFML window as a private attribute — this is where the rendering will occurs. Fig. 2. App object is attached to a viewer (AbstractViewer) and controls a 2D Grid (IGrid)App also defines two important methods : App::run : responsible of the simulation logic App::display : responsible of the display (what we see on the screen).App.h#ifndef APP_H#define APP_Hnamespace sf{\tclass RenderWindow;};namespace env{\tclass IGrid;};namespace viewers{\tclass AbstractViewer;};class App{private:\t// sfml render window\tsf::RenderWindow *_window = nullptr;\t// the 2D grid pointer\tenv::IGrid *_grid = nullptr;\t// a pointer to the viewer\t// this could be a set of viewer actually\t// if we consider component behavior\tviewers::AbstractViewer *_viewer = nullptr;public:\t// theorical width of the environment\t// will match the grid width in terms of number of cells.\tstatic const int DEFAULT_WIDTH;\t// theorical height of the environment.\tstatic const int DEFAULT_HEIGHT;\t// x-resolution of the grid i.e. the x-size of a cell\tstatic const int DEFAULT_RESX;\t// y-resolution of the grid i.e. the y-size of a cell\tstatic const int DEFAULT_RESY;\t// attach window to the app\tvoid setWindow(sf::RenderWindow *);\t// attach a specific viewer\tvoid setViewer(viewers::AbstractViewer *);\t// attach a grid (should have been initialized)\tvoid setGrid(env::IGrid *);\t// return the attached grid\tenv::IGrid *getGrid();\t// return the attached window\tsf::RenderWindow *getWindow();\t// run the application (the logic)\tvoid run();\t// show content (display routines)\tvoid display();\tApp() = default;\tvirtual ~App();};#endif // !APP_H In the App.h file, I have defined static attributes — DEFAULT_WIDTH, DEFAULT_HEIGHT, DEFAULT_RESX, DEFAULT_RESY — to set the default dimensions of a grid in the application.AbstractViewer, GridViewer and the geometry PackageAbstractViewer is meant to be derivated according to what we want to show on the application window — for instance, GridViewer is a specific implementation of AbstractViewer that uses the geometry package to draw the grid lines.AbstractViewer : The Generic Definition of a ViewerAbstractViewer is always attached to an App object — see Fig.3 — and defines a protected abstract method called AbstractViewer::iDraw — implemented in its inherited classes. AbstractViewer::iDraw is called by the public method AbstractViewer::iDisplay when necessary — that will be in the App::display function. Fig. 3. GridViewer is a specific viewer (i.e. inherits from AbstractViewer) in charge of drawing the lines (horizontal and vertical) of the grid. It uses the geometry packageAbstractViewer.h#ifndef ABSTRACTVIEWER_H#define ABSTRACTVIEWER_Hclass App;namespace viewers{\t// AbstractViewer\tclass AbstractViewer\t{\tpublic:\t\t// activate the viewer. If activated, it provides the desired view\t\tvirtual void iActivate();\t\t// deactivate the viewer. Do not display anaything if deactivated\t\tvirtual void iDeactivate();\t\t// return True if the viewer is active\t\tvirtual bool iIsActive() const;\t\t// display function\t\tvirtual void iDisplay();\t\t// attach the application object\t\tvirtual void iSetApp(App *);\t\tvirtual ~AbstractViewer() = default;\t\tAbstractViewer() = default;\tprotected:\t\t// specific draw method (to be concretized in child classes)\t\tvirtual void iDraw() = 0;\t\t// if active the viewer is automatically activated\t\tbool _active = false;\t\t// reference to the attached app\t\tApp *_app;\t};} // namespace viewers#endif // !ABSTRACTVIEWER_H AbstractViewer is always attached to an App object and defines a protected abstract method called AbstractViewer::iDraw.GridViewer : the Grid Lines ViewerFor the exercice, we will define GridViewer as our only viewer, reponsible of displaying the lines of the 2D Grid. The private methods defined in GridViewer are : initialize : to build the set of segments to be displayed drawLine : to call the graphic engine and draw every segment built during the initialization step initialize iDraw : to call the above methods in the right order.GridViewer.h#ifndef GRIDVIEWER_H#define GRIDVIEWER_H#include \"AbstractViewer.h\"#include \"geometry.h\"#include &lt;vector&gt;namespace viewers{\tclass GridViewer : public AbstractViewer\t{\tpublic:\t\tGridViewer() = default;\t\tvirtual void initialize();\t\tvirtual ~GridViewer() = default;\tprotected:\t\tvirtual void iDraw();\tprivate:\t\tvoid drawLines(geometry::ISegmentFunctor &amp;) const;\t\tstd::vector&lt;geometry::Segment&gt; _lines;\t};} // namespace viewers#endif // !GRIDVIEWER_H We define GridViewer as our only viewer, reponsible of displaying the lines of the 2D GridThe geometry PackageHere is the full description of our modest geometry package.It contains the following objects : Point : a 2D point definition, actually a pair of integers Segment : the definition of a segment — a pair of Point ISegmentFunctor : an interface, meant to be realised by functors that applies on Segmentgeometry.h#ifndef GEOMETRY_H#define GEOMETRY_H#include &lt;utility&gt;namespace geometry{\t// definition of a point (typedef is sufficient)\ttypedef std::pair&lt;int, int&gt; Point;\t// definition of a segment (typedef is sufficient)\ttypedef std::pair&lt;Point, Point&gt; Segment;\t// Functor definition to apply on segment\t// We can inherit from this to function\t// to apply display instruction on segments\tclass ISegmentFunctor {\tpublic:\t\t// operator function to apply on each segment \t\t// (should concretized according to the need)\t\tvirtual void operator()(\t\t\tconst Segment&amp; // cell_id\t\t) = 0;\t};}#endif // !GEOMETRY_H Our very modest geometry package contains geometry::Point, geometry::Segment and geometry::ISegmentFunctor.The 2D GridThe most important features of our 2D grid are defined in the IGrid interface — see Fig.4. IGrid is realized by the Grid object which is composed of grid cells — note that the CELL object is defined in IGrid.h. Fig. 4. Grid realizes the interface of a 2D grid, defined in IGridIGrid.h#ifndef IGRID_H#define IGRID_Hnamespace env{\tstruct CELL\t{\t\tint _id; // id of the cell\t\tCELL() = default;\t\tCELL(const CELL &amp;) = default;\t};\tclass IGrid\t{\tpublic:\t\tvirtual ~IGrid() = default;\t\t// returns the width\t\tvirtual int iGetSizeX() const = 0;\t\t// return the height\t\tvirtual int iGetSizeY() const = 0;\t\t// return the number of cells in the grid\t\tvirtual int iGetNumberOfCells() const = 0;\t\t// get the width of a cell (in terms of pixels)\t\tvirtual int iGetResolutionX() const = 0;\t\t// get the height of a cell (in terms of pixels)\t\tvirtual int iGetResolutionY() const = 0;\t\t//-- Test\t\t// relative position of a cell according to its id\t\tvirtual bool iGetCellPosition(\t\t\tconst CELL &amp;, // cell\t\t\tint &amp;, // posx\t\t\tint &amp;, // posy\t\t) const = 0;\t\t// coordinates of a cell accoring to its id\t\tvirtual bool iGetCellCoordinates(\t\t\tconst CELL &amp;, // cell\t\t\tint &amp;, // row_number\t\t\tint &amp; // column_number\t\t) const = 0;\t\t// cell rank of the the cell according\t\t// to its relative position in the grid\t\tvirtual bool iGetCellNumber(\t\t\tint, // row_number\t\t\tint, // column_number\t\t\tCELL &amp;) const = 0;\t\t// the containing cell given the coordinates in the 2D space\t\tvirtual bool iGetContainingCell(\t\t\tint, // posx\t\t\tint, // posy\t\t\tCELL &amp; // cell\t\t) const = 0;\t\t// check if a given point is within a given cell\t\tvirtual bool iIsWithinCell(\t\t\tint, // posx\t\t\tint, // posy\t\t\tconst CELL &amp; // cell\t\t) const = 0;\t\t// initialize the vector of cells, obstacle mask, etc.\t\tvirtual void iInitialize() = 0;\t};} // namespace env#endif // !IGRID_H The features of the 2D grid are defined in IGrid.h — IGrid.h also contains the definition of the CELL object.Grid.h#ifndef GRID_H#define GRID_H#include \"IGrid.h\"#include &lt;vector&gt;namespace env{\tconst int DEFAULT_GRID_SIZEX = 10;\tconst int DEFAULT_GRID_SIZEY = 10;\tconst int DEFAULT_RESOLUTIONX = 1;\tconst int DEFAULT_RESOLUTIONY = 1;\tclass Grid : public IGrid\t{\tpublic:\t\tvirtual ~Grid() = default;\t\tGrid() = default;\t\t//-- Getters\t\tvirtual int iGetSizeX() const;\t\tvirtual int iGetSizeY() const;\t\tvirtual int iGetNumberOfCells() const;\t\tvirtual int iGetResolutionX() const;\t\tvirtual int iGetResolutionY() const;\t\t// Test\t\tvirtual bool iGetCellPosition(const CELL &amp;, int &amp;, int &amp;) const;\t\tvirtual bool iGetCellCoordinates(const CELL &amp;, int &amp;, int &amp;) const;\t\tvirtual bool iGetCellNumber(int, int, CELL &amp;) const;\t\tvirtual bool iGetContainingCell(int, int, CELL &amp;) const;\t\tvirtual bool iIsWithinCell(int, int, const CELL &amp;) const;\t\tvirtual void iInitialize();\t\t//-- Setters\t\tvoid setSizeX(int);\t\tvoid setSizeY(int);\t\tvoid setResolutionX(int);\t\tvoid setResolutionY(int);\tprivate:\t\tint _sizex = DEFAULT_GRID_SIZEX;\t\tint _sizey = DEFAULT_GRID_SIZEY;\t\tint _resolutionx = DEFAULT_RESOLUTIONX;\t\tint _resolutiony = DEFAULT_RESOLUTIONY;\t\tstd::vector&lt;CELL&gt; _cells;\t};} // namespace env#endif // !GRID_H The Grid object realizes the IGrid interface.DemoOur 2D Grid app architecture is now complete! The build strategy is exactly the same as what we have presented in a previous post — we refer the reader to that post for more details. The main file for the demo contains the following :main.cpp#include \"App.h\"#include \"GridViewer.h\"#include \"Grid.h\"#include &lt;thread&gt;#include &lt;SFML/Graphics.hpp&gt;#ifdef __linux__#include &lt;X11/Xlib.h&gt;#endifint main(){#ifdef __linux__ XInitThreads();#endif // -- sfml windows sf::ContextSettings settings; settings.antialiasingLevel = 10; sf::RenderWindow window( sf::VideoMode( (App::DEFAULT_WIDTH*App::DEFAULT_RESX), (App::DEFAULT_HEIGHT*App::DEFAULT_RESY) ), \"SFML 2D Grid\", sf::Style::Titlebar | sf::Style::Close, settings ); window.clear(sf::Color::White); window.setFramerateLimit(120); window.setActive(false); // -- application App app; app.setWindow(&amp;window); //-- grid 2D Grid g; g.setSizeX(App::DEFAULT_WIDTH); g.setSizeY(App::DEFAULT_HEIGHT); g.setResolutionX(App::DEFAULT_RESX); g.setResolutionY(App::DEFAULT_RESY); g.iInitialize(); app.setGrid(&amp;g); //-- viewer GridViewer gviewer; app.setViewer(&amp;gviewer); gviewer.initialize(); gviewer.iActivate(); //-- launch application std::thread rendering_thread(&amp;App::display, &amp;app); app.run(); rendering_thread.join(); return 0;}Configure and BuildThe interested reader can fork the complete source code from here and run the following in a terminal at the root of the project folder :on windows $ cmake -G \"Visual Studio $(Version)\" -S . -B ./build $ cmake --build ./build --config Debug --target app $ ./bin/Debug/appon linux $ mkdir build $ cd build $ cmake -G \"Unix Makefiles\" .. -DCMAKE_BUILD_TYPE=Debug $ cmake --build ./ --target app --config Debug $ ../bin/Debug/appStep by step demo : launching the 2D Grid SFML App from the terminal (built on ubuntu 18.08 with gcc 7.5)You should see a clickable window with a 2D-Grid displayed on it! Enjoy and feel free to send me your feedbacks!References SFML-Dev.org/ GitHub.com/kanmeugne/sfml2dgrid Kanmeugne’s Blog : Building a Portable C++ Graphical App with CMake" }, { "title": "Building a Portable C++ Graphical App with CMake", "url": "/posts/sfml-cmake-windows/", "categories": "modeling & simulation", "tags": "sfml, cmake, c++, modeling, linux, windows", "date": "2020-09-18 00:00:00 +0800", "snippet": "Few months ago, I found an article that explains how to use GoogleTest and GoogleMock — as an external dependency — in a CMake project. Since the approach is amazingly straightforward, I have managed to mimic the paradigm and use SFML — as an external dependency — in a C++ graphical app.Photo by Yannis H on UnsplashThe Idea : build the dependency targets at configuration timeThe main idea is to compile the external project at configure time. This fully integrates it to the build and gives access to all its targets.The original article recommends two sets of definitions to achieve that with googletest : a CMakeLists.txt.in file, which holds the external project references (namely, the official github location) a CMakeLists.txt file, which defines the targets of your application or your library.With SFML as the external project, all I had to do was basically to fill the configuration files with the right url, and define my own target on top of the dependency build. Here is a short presentation of the P.O.C.The WorkspaceA global CMakeLists.txt is located at the root of the folder — for lisibility purposes — as you can see below. It sets the targets folders for the project. The dependency and the graphical app targets are defined in a sub-project.project/├── App│ ├── CMakeLists.txt│ ├── CMakeLists.txt.in│ ├── include│ │ └── App.h│ └── src│ ├── App.cpp│ └── main.cpp├── CMakeLists.txt├── bin├── lib└── build There is a global CMakeLists.txt file to set the targets folders locations. The graphical app target and the SFML dependency configuration are defined in a sub-folder.The Global CMakeList.txt# CMakeList.txt : Upper level configuration filecmake_minimum_required (VERSION 3.10)set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/bin/${CMAKE_BUILD_TYPE}/)set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/lib/${CMAKE_BUILD_TYPE}/)set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/lib/${CMAKE_BUILD_TYPE}/)project (SFMLCMAKE C CXX)add_subdirectory (\"App\") The dependency and the graphical app targets are defined in a sub-project called App — see the next paragraphs for further details.Dependency Location : CMakeList.txt.inCMakeList.txt.in holds the SFML official location data. This file is used during the configuration of graphical app targets.cmake_minimum_required (VERSION 3.8)project(sfml-download NONE)include(ExternalProject)ExternalProject_Add(sfml GIT_REPOSITORY https://github.com/SFML/SFML.git GIT_TAG master SOURCE_DIR \"${CMAKE_SOURCE_DIR}/build/sfml-src\" BINARY_DIR \"${CMAKE_SOURCE_DIR}/build/sfml-build\" CONFIGURE_COMMAND \"\" BUILD_COMMAND \"\" INSTALL_COMMAND \"\" TEST_COMMAND \"\") The SFML official repository is pulled at configuration time.CMakeLists.txt for the targets definitionThis is where the magic is done. This file basically sets the dependency targets to be built at configuration time. Our graphical app target is defined at the end.cmake_minimum_required (VERSION 3.8)project (app C CXX)# build SFML targets ------------configure_file( ${CMAKE_CURRENT_SOURCE_DIR}/CMakeLists.txt.in ${CMAKE_SOURCE_DIR}/build/sfml-download/CMakeLists.txt)execute_process( COMMAND ${CMAKE_COMMAND} -G ${CMAKE_GENERATOR} . RESULT_VARIABLE result WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/build/sfml-download)if(result) message(FATAL_ERROR \"CMake step for sfml failed: ${result}\")endif()execute_process( COMMAND ${CMAKE_COMMAND} --build . RESULT_VARIABLE result WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/build/sfml-download)if(result) message(FATAL_ERROR \"Build step for sfml failed: ${result}\")endif()add_subdirectory( ${CMAKE_SOURCE_DIR}/build/sfml-src ${CMAKE_SOURCE_DIR}/build/sfml-build)#----------------------------------set(SFML_INCLUDE_DIR ${CMAKE_SOURCE_DIR}/build/sfml-build/include/)set(APP_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include/)file(GLOB_RECURSE APP_SRC_FILES ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp)include_directories(${SFML_INCLUDE_DIR} ${APP_INCLUDE_DIR})# app targetadd_executable (app ${APP_SRC_FILES}) if(WIN32 OR WIN64) target_link_libraries(app sfml-window sfml-system sfml-graphics)else() target_link_libraries(app sfml-window sfml-system sfml-graphics pthread X11)endif()source_group(\"src\" FILES ${APP_SRC_FILES})source_group(\"include\" FILES ${APP_INCLUDE_DIR}/*.h) Lines 4 — 27 set the dependency build — you can see how the CMakeList.txt.in file is consumed at line 5. The graphical app target definition begins at line 34 (the configuration is cross-platform – linux and windows).C++ CodeThe main file App/src/main.cpp basically launches two threads : one for the logic of the application — the main thread — and the other, for the display routines (the SFML window should be initialized in the main thread).main.cpp file#include \"App.h\"#include &lt;thread&gt;#include &lt;SFML/Graphics.hpp&gt;#ifdef __linux__#include &lt;X11/Xlib.h&gt;#endifint main(){#ifdef __linux__ // init X threads XInitThreads();#endif sf::ContextSettings settings; settings.antialiasingLevel = 10;\t const unsigned int width = (App::DEFAULT_WIDTH*App::DEFAULT_RESX); const unsigned int height = (App::DEFAULT_HEIGHT*App::DEFAULT_RESY); sf::RenderWindow window ( sf::VideoMode(width, height), \"SFML &amp; CMAKE\", sf::Style::Titlebar | sf::Style::Close, settings ); window.clear(sf::Color::Cyan); window.setFramerateLimit(120); window.setActive(false); // App definition App app; app.setWindow(&amp;window); std::thread rendering_thread(&amp;App::display, &amp;app); app.run(); rendering_thread.join(); return 0;} The main.cpp file launches two threads : one for the logic of the application and another for the display. The source code is cross-platform (Linux and Windows)Configuration and buildIf you fork the full source code from here (to get App.h and App.cpp), you should be able to type the following lines in a terminal at the root of the project folder.On windows # on windows cmake -G \"Visual Studio $(Version)\" -S . -B ./build -DCMAKE_BUILD_TYPE=Debug .. cmake --build ./build --config Debug --target appOn Linux # on linux mkdir build cd build cmake -G \"Unix Makefiles\" .. -DCMAKE_BUILD_TYPE=Debug cmake --build ./ --target app --config Debug Note: For linux user, you might need to check this first.RunYou should be able to launch the executable located in the bin folder and see a nice (and clickable) cyan window../bin/Debug/appStep by step Demo : launching the SFML app from the terminal. In this tutorial the program is built on ubuntu 18.08 with gcc 7.5Smile! Now you are ready to take your graphical app wherever you want. Enjoy and feel free to send me your feedbacks!References Cmake.org Googletest crascit: CMake-Gtest SFML-Dev.org : compile with cmake GitHub.com/kanmeugne/sfmlcmake" } ]
