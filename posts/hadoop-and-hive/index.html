<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Hadoop : query data from a self-hosted hadoop cluster" /><meta name="author" content="Patrick S. Kanmeugne" /><meta property="og:locale" content="en" /><meta name="description" content="Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the answer to the unevitable question we face one day or another as we live in a data age – which is : how do we process tons of data efficiently ? It is not just about storage, but also, and even more, about implementing data processing models that can provide insights to decision makers in a competitive world – where everything has to be fast and resilient." /><meta property="og:description" content="Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the answer to the unevitable question we face one day or another as we live in a data age – which is : how do we process tons of data efficiently ? It is not just about storage, but also, and even more, about implementing data processing models that can provide insights to decision makers in a competitive world – where everything has to be fast and resilient." /><link rel="canonical" href="https://kanmeugne.github.io/posts/hadoop-and-hive/" /><meta property="og:url" content="https://kanmeugne.github.io/posts/hadoop-and-hive/" /><meta property="og:site_name" content="Kanmeugne’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-05-15T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Hadoop : query data from a self-hosted hadoop cluster" /><meta name="twitter:site" content="@patricksimok" /><meta name="twitter:creator" content="@patricksimok" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Patrick S. Kanmeugne","url":"http://kanmeugne.github.io/"},"dateModified":"2025-09-30T00:05:31+08:00","datePublished":"2025-05-15T00:00:00+08:00","description":"Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the answer to the unevitable question we face one day or another as we live in a data age – which is : how do we process tons of data efficiently ? It is not just about storage, but also, and even more, about implementing data processing models that can provide insights to decision makers in a competitive world – where everything has to be fast and resilient.","headline":"Hadoop : query data from a self-hosted hadoop cluster","mainEntityOfPage":{"@type":"WebPage","@id":"https://kanmeugne.github.io/posts/hadoop-and-hive/"},"url":"https://kanmeugne.github.io/posts/hadoop-and-hive/"}</script><title>Hadoop : query data from a self-hosted hadoop cluster | Kanmeugne's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Kanmeugne's Blog"><meta name="application-name" content="Kanmeugne's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/0.jpeg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Kanmeugne's Blog</a></div><div class="site-subtitle font-italic">stigmergy matters</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/kanmeugne" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/patricksimok" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['patrick.simokanmeugne','outlook.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Hadoop : query data from a self-hosted hadoop cluster</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Hadoop : query data from a self-hosted hadoop cluster</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1747238400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 15, 2025 </em> </span> <span> Updated <em class="" data-ts="1759161931" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Sep 29, 2025 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="http://kanmeugne.github.io/">Patrick S. Kanmeugne</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1192 words"> <em>6 min</em> read</span></div></div></div><div class="post-content"><p>Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the answer to the unevitable question we face one day or another as we live in a data age – which is : <em>how do we process tons of data efficiently ?</em> It is not just about storage, but also, and even more, about implementing data processing models that can provide insights to decision makers in a competitive world – where everything has to be fast and resilient.</p><p><img data-src="/images/hive.png" alt="alt text" width="50%" data-proofer-ignore> <em>Kanmeugne’s Blog – Hadoop : query data from a self-hosted hadoop cluster</em></p><p>There are many important concepts to know in order to <a href="https://www.oreilly.com/library/view/hadoop-the-definitive/9781491901687/" title="Hadoop: The Definitive Guide, 4th Edition, O'Reilly Media, Inc.">understand the hadoop framework</a> – in this tutorial we will focus on 3 of them :</p><ul><li><strong>Map/Reduce model</strong> : a programming model for data processing, inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal. Map/Reduce program can be written in several popular languages – Java, Python, Ruby etc. – or wrapped using distributed tools, like <a href="https://hive.apache.org/" title="The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL.">Apache Hive</a>, built on top of the hadoop platform.<li><strong>HDFS</strong> : Hadoop comes with a distributed filesystem called HDFS, which stands for Hadoop Distributed Filesystem. <em>HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware</em><li><strong>Apache YARN</strong> (Yet Another Resource Negotiator) is Hadoop’s cluster resource management system. <em>YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Instead, users write to higher-level APIs provided by distributed computing frameworks, which themselves are built on YARN and hide the resource management details from the user</em></ul><p>To learn more about hadoop platform, the interested reader could have a look at <a href="https://www.oreilly.com/library/view/hadoop-the-definitive/9781491901687/" title="Hadoop: The Definitive Guide, 4th Edition, O'Reilly Media, Inc.">this excellent book</a>, published by Oreilly.</p><h2 id="how-to"><span class="mr-2">How To</span><a href="#how-to" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Now let’s jump to the hands on tutorial. I will mostly focus on high level operations – data i/o and analysis – for the interested users could easily find more specific tutorials on low-level operations. Here, we will rapidly set up a custom cluster using docker compose, add some data in the corresponding <strong>HDFS</strong> filesystem, and process the data using <a href="https://hive.apache.org/" title="The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL."><strong>Hive</strong></a>.</p><h3 id="set-the-self-hosted-cluster"><span class="mr-2">Set the self-hosted cluster</span><a href="#set-the-self-hosted-cluster" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The full code for this tutorial is available <a href="https://github.com/kanmeugne/modern-data-architectures.git" title="Kanmeugne's Blog (github) : Tutorials on modern data architectures">from github</a>, you just have to pull and run :</p><ul><li><strong>Clone and deploy the Hadoop Docker setup:</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nv">$ </span>git clone https://github.com/kanmeugne/modern-data-architectures.git
<span class="nv">$ </span><span class="nb">cd </span>modern-data-architectures/handzon-hadoop-hive 
<span class="nv">$ </span>docker-compose up <span class="nt">-d</span>
</pre></table></code></div></div><p>This launches namenodes, datanodes, and supporting services in containers. It also creates a hive server, to create and query data in a hdfs-compatible database.</p><li><strong>Check running containers:</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">$ </span>docker ps
</pre></table></code></div></div><p>All Hadoop containers (namenode, datanode(s), etc.) should be listed.</p><li><strong>Check hdfs filesystem from inside the name node:</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre><td class="rouge-code"><pre><span class="nv">$ </span>docker <span class="nb">exec</span> &lt;namenode&gt; hdfs dfsadmin <span class="nt">-report</span> 
<span class="c"># this command lists all live DataNodes connected to the cluster.</span>
Configured Capacity: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> GB<span class="o">)</span>
Present Capacity: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> GB<span class="o">)</span>
DFS Remaining: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> GB<span class="o">)</span>
DFS Used: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> MB<span class="o">)</span>
DFS Used%: 0.01%
Replicated Blocks:
  Under replicated blocks: 6
  Blocks with corrupt replicas: 0
  Missing blocks: 0
  Missing blocks <span class="o">(</span>with replication <span class="nb">factor </span>1<span class="o">)</span>: 0
  Low redundancy blocks with highest priority to recover: 6
  Pending deletion blocks: 0
Erasure Coded Block Groups: 
  Low redundancy block <span class="nb">groups</span>: 0
  Block <span class="nb">groups </span>with corrupt internal blocks: 0
  Missing block <span class="nb">groups</span>: 0
  Low redundancy blocks with highest priority to recover: 0
  Pending deletion blocks: 0
<span class="nt">-------------------------------------------------</span>
Live datanodes <span class="o">(</span>1<span class="o">)</span>:

Name: <span class="k">***</span> <span class="o">(</span>datanode.handzon-hadoop-hive_hadoop_network<span class="o">)</span>
Hostname: e20decb5140e
Decommission Status : Normal
Configured Capacity: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> GB<span class="o">)</span>
DFS Used: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> MB<span class="o">)</span>
Non DFS Used: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> GB<span class="o">)</span>
DFS Remaining: <span class="k">***</span> <span class="o">(</span><span class="k">***</span> GB<span class="o">)</span>
DFS Used%: 0.00%
DFS Remaining%: 5.85%
Configured Cache Capacity: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Used: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Remaining: 0 <span class="o">(</span>0 B<span class="o">)</span>
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Fri May 09 20:23:16 UTC 2025
Last Block Report: Fri May 09 20:17:40 UTC 2025
Num of Blocks: 6

  ...
</pre></table></code></div></div></ul><h3 id="add-data-in-the-cluster"><span class="mr-2">Add data in the cluster</span><a href="#add-data-in-the-cluster" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Let’s add some data in the distributed filesystem:</p><ul><li><strong>Copy your CSV file into the namenode container:</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nv">$ </span>curl <span class="nt">-L</span> <span class="nt">-o</span> movieratings.csv https://files.grouplens.org/datasets/movielens/ml-100k/u.data
<span class="nv">$ </span>docker <span class="nb">cp </span>movieratings.csv &lt;namenode&gt;:/tmp/ <span class="c"># on the docker host</span>
</pre></table></code></div></div><blockquote><p>The <a href="https://grouplens.org/datasets/movielens/100k/" title="MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.">dataset</a> comes from <a href="https://grouplens.org/about/what-is-grouplens/">GroupLens</a>, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities specializing in recommender systems, online communities, mobile and ubiquitous technologies, digital libraries, and local geographic information systems.</p></blockquote><li><strong>Load the CSV into an HDFS folder within the container:</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nv">$ </span>docker <span class="nb">exec</span> &lt;namenode&gt; hdfs dfs <span class="nt">-mkdir</span> <span class="nt">-p</span> /input
<span class="nv">$ </span>docker <span class="nb">exec</span> &lt;namenode&gt; hdfs dfs <span class="nt">-put</span> /tmp/movieratings.csv /input/ <span class="c"># in the docker</span>
</pre></table></code></div></div></ul><h3 id="explore-your-data-with-hive"><span class="mr-2">Explore your data with Hive</span><a href="#explore-your-data-with-hive" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Using <a href="https://hive.apache.org/" title="The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL.">Hive</a> you can explore data with SQL-like queries :</p><ul><li><strong>Access the Hive service container</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">$ </span>docker <span class="nb">exec</span> <span class="nt">-it</span> &lt;hive-server&gt; bash <span class="c"># `&lt;hive-server&gt;` is the name of your hive server</span>
</pre></table></code></div></div><li><strong>Create an external table from the HDFS file:</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="c"># in the docker</span>
<span class="nv">$ </span>beeline <span class="nt">-u</span> jdbc:hive2://localhost:10000
...
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive <span class="o">(</span>version 2.3.2<span class="o">)</span>
Driver: Hive JDBC <span class="o">(</span>version 2.3.2<span class="o">)</span>
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 2.3.2 by Apache Hive
...
</pre></table></code></div></div><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>0: jdbc:hive2://localhost:10000&gt;
<span class="c"># This tells Hive to use the CSV at `/input` in HDFS as the data source.</span>
CREATE EXTERNAL TABLE IF NOT EXISTS movieratins <span class="o">(</span>
  user_id STRING,
  movie_id STRING,
  rating FLOAT,
  datation STRING
<span class="o">)</span> ROW FORMAT
DELIMITED FIELDS TERMINATED BY <span class="s1">'\t'</span>
STORED AS TEXTFILE
LOCATION <span class="s1">'/input'</span><span class="p">;</span> <span class="c"># hit enter</span>
</pre></table></code></div></div><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="c"># You should see this message after you hit `enter`</span>
No rows affected <span class="o">(</span>1.629 seconds<span class="o">)</span>
</pre></table></code></div></div><li><strong>Query the created table</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>0: hive2://localhost:10000&gt; <span class="k">select</span> <span class="k">*</span> from movieratings limit 4<span class="p">;</span> <span class="c"># hit enter</span>
</pre></table></code></div></div><pre><code class="language-verbatim">+----------+-----------+---------+-----------+
| user_id  | movie_id  | rating  | datation  |
+----------+-----------+---------+-----------+
| 196      | 242       | 3.0     | 881250949 |
| 186      | 302       | 3.0     | 891717742 |
| 305      | 451       | 3.0     | 886324817 |
| 6        | 86        | 3.0     | 883603013 |
+----------+-----------+---------+-----------+
</code></pre><li><strong>Do some analytics using sql queries on the hive table</strong><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c"># compute the average rating per movie</span>
0: jdbc:hive2://localhost:10000&gt; 
SELECT movie_id,
AVG<span class="o">(</span>rating<span class="o">)</span> as rating
FROM movierating
GROU BY movie_id
ORDER BY LENGTH<span class="o">(</span>movie_id<span class="o">)</span>, movie_id
LIMIT 10<span class="p">;</span>
</pre></table></code></div></div><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="c"># results</span>
+-----------+---------------------+
| movie_id  |       rating        |
+-----------+---------------------+
| 1         | 3.8783185840707963  |
| 2         | 3.2061068702290076  |
| 3         | 3.033333333333333   |
| 4         | 3.550239234449761   |
| 5         | 3.302325581395349   |
| 6         | 3.576923076923077   |
| 7         | 3.798469387755102   |
| 8         | 3.9954337899543377  |
| 9         | 3.8963210702341136  |
| 10        | 3.831460674157303   |
+-----------+---------------------+
10 rows selected <span class="o">(</span>2.909 seconds<span class="o">)</span>
</pre></table></code></div></div><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>0: jdbc:hive2://localhost:10000&gt; <span class="o">!</span>quit
</pre></table></code></div></div><li><strong>Voilà! You can add nodes and compare execution times</strong></ul><p>Feel free to pull this repo and to send me your comments/remarks.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/microservcies/'>microservcies</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/apache/" class="post-tag no-text-decoration" >apache</a> <a href="/tags/superset/" class="post-tag no-text-decoration" >superset</a> <a href="/tags/postgresql/" class="post-tag no-text-decoration" >postgresql</a> <a href="/tags/docker/" class="post-tag no-text-decoration" >docker</a> <a href="/tags/solution-architect/" class="post-tag no-text-decoration" >solution architect</a> <a href="/tags/hive/" class="post-tag no-text-decoration" >hive</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Hadoop+%3A+query+data+from+a+self-hosted+hadoop+cluster+-+Kanmeugne%27s+Blog&url=https%3A%2F%2Fkanmeugne.github.io%2Fposts%2Fhadoop-and-hive%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Hadoop+%3A+query+data+from+a+self-hosted+hadoop+cluster+-+Kanmeugne%27s+Blog&u=https%3A%2F%2Fkanmeugne.github.io%2Fposts%2Fhadoop-and-hive%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fkanmeugne.github.io%2Fposts%2Fhadoop-and-hive%2F&text=Hadoop+%3A+query+data+from+a+self-hosted+hadoop+cluster+-+Kanmeugne%27s+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/hadoop-and-hive/">Hadoop : query data from a self-hosted hadoop cluster</a><li><a href="/posts/2d-grid-obstacles/">2D Grid with obstacles</a><li><a href="/posts/pheromons-evaporation/">Pheromon evaporation on a 2D Grid</a><li><a href="/posts/sfml-2d-grid/">Drawing a 2D Grid with SFML</a><li><a href="/posts/postgresql-and-apache-superset/">Apache Superset and Postgresql : connecting your database to a powerful data visualisation engine</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/c/">c++</a> <a class="post-tag" href="/tags/cmake/">cmake</a> <a class="post-tag" href="/tags/modeling/">modeling</a> <a class="post-tag" href="/tags/sfml/">sfml</a> <a class="post-tag" href="/tags/apache/">apache</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/simulation/">simulation</a> <a class="post-tag" href="/tags/solution-architect/">solution architect</a> <a class="post-tag" href="/tags/hive/">hive</a> <a class="post-tag" href="/tags/ipython/">ipython</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/postgresql-and-apache-superset/"><div class="card-body"> <em class="small" data-ts="1746028800" data-df="ll" > May 1, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Apache Superset and Postgresql : connecting your database to a powerful data visualisation engine</h3><div class="text-muted small"><p> Data is only as valuable as the insights you can extract from it — and in today’s world, those insights need to be fast, interactive, and visually compelling. Kanmeugne’s Blog : Apache Superset a...</p></div></div></a></div><div class="card"> <a href="/posts/hadoop-map-reduce-using-python/"><div class="card-body"> <em class="small" data-ts="1758470400" data-df="ll" > Sep 22, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Hadoop : Map/Reduce Using Python</h3><div class="text-muted small"><p> Hadoop is an affordable, reliable and scalable platform for big data storage and analysis – it runs on commodity hardware and it is open source. Technically speaking, the Hadoop platform is the ans...</p></div></div></a></div><div class="card"> <a href="/posts/setting-up-virtual-environments-in-python/"><div class="card-body"> <em class="small" data-ts="1651593600" data-df="ll" > May 4, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Setting Up Your python Environment (II)</h3><div class="text-muted small"><p> Dans un article précédent, j’expliquais en 3 étapes comment mettre en place un environnement de programmation en Python. Il existe plusieurs options d’installation en réalité (anaconda, winpython, ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/postgresql-and-apache-superset/" class="btn btn-outline-primary" prompt="Older"><p>Apache Superset and Postgresql : connecting your database to a powerful data visualisation engine</p></a> <a href="/posts/hadoop-map-reduce-using-python/" class="btn btn-outline-primary" prompt="Newer"><p>Hadoop : Map/Reduce Using Python</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "kanmeugne/kanmeugne.github.io", "data-repo-id": "MDEwOlJlcG9zaXRvcnkyOTc1NjcwMjA=", "data-category": "General", "data-category-id": "DIC_kwDOEbyDLM4CPQDi", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/patricksimok">Patrick S. Kanmeugne</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/c/">c++</a> <a class="post-tag" href="/tags/cmake/">cmake</a> <a class="post-tag" href="/tags/modeling/">modeling</a> <a class="post-tag" href="/tags/sfml/">sfml</a> <a class="post-tag" href="/tags/apache/">apache</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/simulation/">simulation</a> <a class="post-tag" href="/tags/solution-architect/">solution architect</a> <a class="post-tag" href="/tags/hive/">hive</a> <a class="post-tag" href="/tags/ipython/">ipython</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-KFTQ8M8D5N"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-KFTQ8M8D5N'); }); </script>
